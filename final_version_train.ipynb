{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9bf86-a523-434e-9dbc-60a78bca29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## submit version\n",
    "# 1.run warmup block2\n",
    "# 2. run train block3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e7cdc6-82fc-4091-a465-0a96069e9971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/tPINN_resNet_FFT_ELU_w=1_layers=[2, 64, 64, 64, 64, 64, 64, 64, 4]/train0.log\n",
      "Epoch 0: New best model saved with Loss 8295176.0\n",
      "Epoch 3: New best model saved with Loss 1224358.5\n",
      "Epoch 6: New best model saved with Loss 251891.9375\n",
      "Epoch 14: New best model saved with Loss 235517.25\n",
      "Epoch 17: New best model saved with Loss 222118.140625\n",
      "Epoch 18: New best model saved with Loss 208561.96875\n",
      "Epoch 21: New best model saved with Loss 148219.8125\n",
      "Epoch 22: New best model saved with Loss 131085.484375\n",
      "Epoch 26: New best model saved with Loss 124828.46875\n",
      "Epoch 27: New best model saved with Loss 91396.234375\n",
      "Epoch 32: New best model saved with Loss 74780.75\n",
      "Epoch 37: New best model saved with Loss 67460.75\n",
      "Epoch 38: New best model saved with Loss 64699.1796875\n",
      "Epoch 39: New best model saved with Loss 59710.875\n",
      "Epoch 40: New best model saved with Loss 55384.734375\n",
      "Epoch 44: New best model saved with Loss 55030.15625\n",
      "Epoch 45: New best model saved with Loss 47442.76953125\n",
      "Epoch 46: New best model saved with Loss 46933.70703125\n",
      "Epoch 48: New best model saved with Loss 46468.3515625\n",
      "Epoch 51: New best model saved with Loss 46357.53125\n",
      "Epoch 52: New best model saved with Loss 45751.28125\n",
      "Epoch 53: New best model saved with Loss 42884.33203125\n",
      "Epoch 55: New best model saved with Loss 42615.2109375\n",
      "Epoch 56: New best model saved with Loss 41915.81640625\n",
      "Epoch 58: New best model saved with Loss 41202.98828125\n",
      "Epoch 59: New best model saved with Loss 40163.578125\n",
      "Epoch 60: New best model saved with Loss 39744.76953125\n",
      "Epoch 61: New best model saved with Loss 38269.87109375\n",
      "Epoch 62: New best model saved with Loss 37950.82421875\n",
      "Epoch 64: New best model saved with Loss 37847.859375\n",
      "Epoch 65: New best model saved with Loss 37767.2734375\n",
      "Epoch 67: New best model saved with Loss 37138.734375\n",
      "Epoch 68: New best model saved with Loss 36515.28515625\n",
      "Epoch 69: New best model saved with Loss 36491.58984375\n",
      "Epoch 70: New best model saved with Loss 36255.93359375\n",
      "Epoch 71: New best model saved with Loss 35915.87109375\n",
      "Epoch 73: New best model saved with Loss 35913.04296875\n",
      "Epoch 74: New best model saved with Loss 35431.203125\n",
      "Epoch 75: New best model saved with Loss 35004.140625\n",
      "Epoch 76: New best model saved with Loss 34854.75390625\n",
      "Epoch 77: New best model saved with Loss 34689.44921875\n",
      "Epoch 78: New best model saved with Loss 34431.859375\n",
      "Epoch 79: New best model saved with Loss 34292.4609375\n",
      "Epoch 80: New best model saved with Loss 34290.4296875\n",
      "Epoch 81: New best model saved with Loss 34221.86328125\n",
      "Epoch 82: New best model saved with Loss 34001.046875\n",
      "Epoch 83: New best model saved with Loss 33755.2421875\n",
      "Epoch 84: New best model saved with Loss 33623.66796875\n",
      "Epoch 85: New best model saved with Loss 33602.953125\n",
      "Epoch 86: New best model saved with Loss 33595.86328125\n",
      "Epoch 87: New best model saved with Loss 33524.8359375\n",
      "Epoch 88: New best model saved with Loss 33383.671875\n",
      "Epoch 89: New best model saved with Loss 33212.53125\n",
      "Epoch 90: New best model saved with Loss 33061.48828125\n",
      "Epoch 91: New best model saved with Loss 32957.2734375\n",
      "Epoch 92: New best model saved with Loss 32898.03125\n",
      "Epoch 93: New best model saved with Loss 32865.40234375\n",
      "Epoch 94: New best model saved with Loss 32837.65234375\n",
      "Epoch 95: New best model saved with Loss 32805.83203125\n",
      "Epoch 96: New best model saved with Loss 32773.43359375\n",
      "Epoch 97: New best model saved with Loss 32767.2421875\n",
      "Epoch 129: New best model saved with Loss 32021.486328125\n",
      "Epoch 133: New best model saved with Loss 31384.087890625\n",
      "Epoch 135: New best model saved with Loss 31366.8984375\n",
      "Epoch 137: New best model saved with Loss 31294.228515625\n",
      "Epoch 139: New best model saved with Loss 30882.42578125\n",
      "Epoch 143: New best model saved with Loss 30796.6796875\n",
      "Epoch 145: New best model saved with Loss 30640.41796875\n",
      "Epoch 146: New best model saved with Loss 30546.26171875\n",
      "Epoch 147: New best model saved with Loss 30516.90234375\n",
      "Epoch 148: New best model saved with Loss 30391.234375\n",
      "Epoch 149: New best model saved with Loss 30262.37890625\n",
      "Epoch 150: New best model saved with Loss 30218.7734375\n",
      "Epoch 151: New best model saved with Loss 30164.580078125\n",
      "Epoch 152: New best model saved with Loss 30009.6328125\n",
      "Epoch 153: New best model saved with Loss 29984.177734375\n",
      "Epoch 154: New best model saved with Loss 29907.46875\n",
      "Epoch 155: New best model saved with Loss 29783.65234375\n",
      "Epoch 156: New best model saved with Loss 29734.64453125\n",
      "Epoch 157: New best model saved with Loss 29679.359375\n",
      "Epoch 158: New best model saved with Loss 29586.23828125\n",
      "Epoch 159: New best model saved with Loss 29500.0390625\n",
      "Epoch 160: New best model saved with Loss 29491.234375\n",
      "Epoch 161: New best model saved with Loss 29362.669921875\n",
      "Epoch 162: New best model saved with Loss 29340.3359375\n",
      "Epoch 163: New best model saved with Loss 29248.64453125\n",
      "Epoch 164: New best model saved with Loss 29227.8828125\n",
      "Epoch 165: New best model saved with Loss 29099.1171875\n",
      "Epoch 167: New best model saved with Loss 28980.205078125\n",
      "Epoch 168: New best model saved with Loss 28970.6328125\n",
      "Epoch 169: New best model saved with Loss 28868.21875\n",
      "Epoch 170: New best model saved with Loss 28849.6796875\n",
      "Epoch 171: New best model saved with Loss 28744.23828125\n",
      "Epoch 172: New best model saved with Loss 28725.99609375\n",
      "Epoch 173: New best model saved with Loss 28637.09765625\n",
      "Epoch 174: New best model saved with Loss 28587.87890625\n",
      "Epoch 175: New best model saved with Loss 28527.33203125\n",
      "Epoch 176: New best model saved with Loss 28462.591796875\n",
      "Epoch 177: New best model saved with Loss 28406.09765625\n",
      "Epoch 178: New best model saved with Loss 28337.87109375\n",
      "Epoch 179: New best model saved with Loss 28289.64453125\n",
      "Epoch 180: New best model saved with Loss 28208.78515625\n",
      "Epoch 181: New best model saved with Loss 28167.04296875\n",
      "Epoch 182: New best model saved with Loss 28088.9453125\n",
      "Epoch 183: New best model saved with Loss 28035.15234375\n",
      "Epoch 184: New best model saved with Loss 27969.482421875\n",
      "Epoch 185: New best model saved with Loss 27906.3671875\n",
      "Epoch 186: New best model saved with Loss 27844.341796875\n",
      "Epoch 187: New best model saved with Loss 27777.197265625\n",
      "Epoch 188: New best model saved with Loss 27718.625\n",
      "Epoch 189: New best model saved with Loss 27646.322265625\n",
      "Epoch 190: New best model saved with Loss 27589.07421875\n",
      "Epoch 191: New best model saved with Loss 27518.44921875\n",
      "Epoch 192: New best model saved with Loss 27454.513671875\n",
      "Epoch 193: New best model saved with Loss 27389.890625\n",
      "Epoch 194: New best model saved with Loss 27319.919921875\n",
      "Epoch 195: New best model saved with Loss 27256.43359375\n",
      "Epoch 196: New best model saved with Loss 27185.966796875\n",
      "Epoch 197: New best model saved with Loss 27120.07421875\n",
      "Epoch 198: New best model saved with Loss 27050.224609375\n",
      "Epoch 199: New best model saved with Loss 26981.486328125\n",
      "Epoch 200: New best model saved with Loss 26912.400390625\n",
      "Epoch 201: New best model saved with Loss 26840.53125\n",
      "Epoch 202: New best model saved with Loss 26771.75390625\n",
      "Epoch 203: New best model saved with Loss 26698.53125\n",
      "Epoch 204: New best model saved with Loss 26627.296875\n",
      "Epoch 205: New best model saved with Loss 26555.25390625\n",
      "Epoch 206: New best model saved with Loss 26480.908203125\n",
      "Epoch 207: New best model saved with Loss 26408.96484375\n",
      "Epoch 208: New best model saved with Loss 26334.185546875\n",
      "Epoch 209: New best model saved with Loss 26260.10546875\n",
      "Epoch 210: New best model saved with Loss 26185.921875\n",
      "Epoch 211: New best model saved with Loss 26109.9921875\n",
      "Epoch 212: New best model saved with Loss 26035.171875\n",
      "Epoch 213: New best model saved with Loss 25958.97265625\n",
      "Epoch 214: New best model saved with Loss 25882.6328125\n",
      "Epoch 215: New best model saved with Loss 25806.58203125\n",
      "Epoch 216: New best model saved with Loss 25729.45703125\n",
      "Epoch 217: New best model saved with Loss 25652.7109375\n",
      "Epoch 218: New best model saved with Loss 25575.365234375\n",
      "Epoch 219: New best model saved with Loss 25497.453125\n",
      "Epoch 220: New best model saved with Loss 25419.533203125\n",
      "Epoch 221: New best model saved with Loss 25340.76171875\n",
      "Epoch 222: New best model saved with Loss 25261.8515625\n",
      "Epoch 223: New best model saved with Loss 25182.6328125\n",
      "Epoch 224: New best model saved with Loss 25102.70703125\n",
      "Epoch 225: New best model saved with Loss 25022.64453125\n",
      "Epoch 226: New best model saved with Loss 24942.109375\n",
      "Epoch 227: New best model saved with Loss 24861.01171875\n",
      "Epoch 228: New best model saved with Loss 24779.853515625\n",
      "Epoch 229: New best model saved with Loss 24698.15625\n",
      "Epoch 230: New best model saved with Loss 24616.19140625\n",
      "Epoch 231: New best model saved with Loss 24534.140625\n",
      "Epoch 232: New best model saved with Loss 24451.775390625\n",
      "Epoch 233: New best model saved with Loss 24369.2890625\n",
      "Epoch 234: New best model saved with Loss 24286.697265625\n",
      "Epoch 235: New best model saved with Loss 24203.849609375\n",
      "Epoch 236: New best model saved with Loss 24121.02734375\n",
      "Epoch 237: New best model saved with Loss 24038.177734375\n",
      "Epoch 238: New best model saved with Loss 23955.193359375\n",
      "Epoch 239: New best model saved with Loss 23872.234375\n",
      "Epoch 240: New best model saved with Loss 23789.25\n",
      "Epoch 241: New best model saved with Loss 23706.3203125\n",
      "Epoch 242: New best model saved with Loss 23623.53125\n",
      "Epoch 243: New best model saved with Loss 23540.74609375\n",
      "Epoch 244: New best model saved with Loss 23458.14453125\n",
      "Epoch 245: New best model saved with Loss 23375.71875\n",
      "Epoch 246: New best model saved with Loss 23293.625\n",
      "Epoch 247: New best model saved with Loss 23211.873046875\n",
      "Epoch 248: New best model saved with Loss 23130.34375\n",
      "Epoch 249: New best model saved with Loss 23049.234375\n",
      "Epoch 250: New best model saved with Loss 22968.5546875\n",
      "Epoch 251: New best model saved with Loss 22888.5390625\n",
      "Epoch 252: New best model saved with Loss 22809.2265625\n",
      "Epoch 253: New best model saved with Loss 22730.55859375\n",
      "Epoch 254: New best model saved with Loss 22652.72265625\n",
      "Epoch 255: New best model saved with Loss 22575.92578125\n",
      "Epoch 256: New best model saved with Loss 22500.21484375\n",
      "Epoch 257: New best model saved with Loss 22425.55859375\n",
      "Epoch 258: New best model saved with Loss 22352.134765625\n",
      "Epoch 259: New best model saved with Loss 22279.830078125\n",
      "Epoch 260: New best model saved with Loss 22208.72265625\n",
      "Epoch 261: New best model saved with Loss 22138.75\n",
      "Epoch 262: New best model saved with Loss 22070.04296875\n",
      "Epoch 263: New best model saved with Loss 22002.6640625\n",
      "Epoch 264: New best model saved with Loss 21936.689453125\n",
      "Epoch 265: New best model saved with Loss 21872.154296875\n",
      "Epoch 266: New best model saved with Loss 21808.95703125\n",
      "Epoch 267: New best model saved with Loss 21747.34375\n",
      "Epoch 268: New best model saved with Loss 21687.23046875\n",
      "Epoch 269: New best model saved with Loss 21628.70703125\n",
      "Epoch 270: New best model saved with Loss 21571.77734375\n",
      "Epoch 271: New best model saved with Loss 21516.4140625\n",
      "Epoch 272: New best model saved with Loss 21462.5859375\n",
      "Epoch 273: New best model saved with Loss 21410.296875\n",
      "Epoch 274: New best model saved with Loss 21359.46484375\n",
      "Epoch 275: New best model saved with Loss 21310.103515625\n",
      "Epoch 276: New best model saved with Loss 21262.14453125\n",
      "Epoch 277: New best model saved with Loss 21215.5859375\n",
      "Epoch 278: New best model saved with Loss 21170.47265625\n",
      "Epoch 279: New best model saved with Loss 21126.66796875\n",
      "Epoch 280: New best model saved with Loss 21084.23046875\n",
      "Epoch 281: New best model saved with Loss 21043.015625\n",
      "Epoch 282: New best model saved with Loss 21003.078125\n",
      "Epoch 283: New best model saved with Loss 20964.314453125\n",
      "Epoch 284: New best model saved with Loss 20926.67578125\n",
      "Epoch 285: New best model saved with Loss 20890.19140625\n",
      "Epoch 286: New best model saved with Loss 20854.85546875\n",
      "Epoch 287: New best model saved with Loss 20820.642578125\n",
      "Epoch 288: New best model saved with Loss 20787.48046875\n",
      "Epoch 289: New best model saved with Loss 20755.37109375\n",
      "Epoch 290: New best model saved with Loss 20724.234375\n",
      "Epoch 291: New best model saved with Loss 20694.07421875\n",
      "Epoch 292: New best model saved with Loss 20664.83984375\n",
      "Epoch 293: New best model saved with Loss 20636.505859375\n",
      "Epoch 294: New best model saved with Loss 20608.9453125\n",
      "Epoch 295: New best model saved with Loss 20582.21484375\n",
      "Epoch 296: New best model saved with Loss 20556.2421875\n",
      "Epoch 297: New best model saved with Loss 20530.875\n",
      "Epoch 298: New best model saved with Loss 20506.32421875\n",
      "Epoch 299: New best model saved with Loss 20482.34765625\n",
      "Epoch 300: New best model saved with Loss 20458.921875\n",
      "Epoch 301: New best model saved with Loss 20436.15625\n",
      "Epoch 302: New best model saved with Loss 20413.7578125\n",
      "Epoch 303: New best model saved with Loss 20391.97265625\n",
      "Epoch 304: New best model saved with Loss 20370.609375\n",
      "Epoch 305: New best model saved with Loss 20349.701171875\n",
      "Epoch 306: New best model saved with Loss 20329.28515625\n",
      "Epoch 307: New best model saved with Loss 20309.24609375\n",
      "Epoch 308: New best model saved with Loss 20289.48828125\n",
      "Epoch 309: New best model saved with Loss 20270.12890625\n",
      "Epoch 310: New best model saved with Loss 20251.10546875\n",
      "Epoch 311: New best model saved with Loss 20232.3671875\n",
      "Epoch 312: New best model saved with Loss 20213.83203125\n",
      "Epoch 313: New best model saved with Loss 20195.6953125\n",
      "Epoch 314: New best model saved with Loss 20177.7109375\n",
      "Epoch 315: New best model saved with Loss 20160.0\n",
      "Epoch 316: New best model saved with Loss 20142.4375\n",
      "Epoch 317: New best model saved with Loss 20125.13671875\n",
      "Epoch 318: New best model saved with Loss 20108.021484375\n",
      "Epoch 319: New best model saved with Loss 20091.14453125\n",
      "Epoch 320: New best model saved with Loss 20074.35546875\n",
      "Epoch 321: New best model saved with Loss 20057.85546875\n",
      "Epoch 322: New best model saved with Loss 20041.35546875\n",
      "Epoch 323: New best model saved with Loss 20025.140625\n",
      "Epoch 324: New best model saved with Loss 20008.982421875\n",
      "Epoch 325: New best model saved with Loss 19992.94140625\n",
      "Epoch 326: New best model saved with Loss 19977.078125\n",
      "Epoch 327: New best model saved with Loss 19961.28515625\n",
      "Epoch 328: New best model saved with Loss 19945.619140625\n",
      "Epoch 329: New best model saved with Loss 19930.060546875\n",
      "Epoch 330: New best model saved with Loss 19914.640625\n",
      "Epoch 331: New best model saved with Loss 19899.29296875\n",
      "Epoch 332: New best model saved with Loss 19884.0546875\n",
      "Epoch 333: New best model saved with Loss 19868.87890625\n",
      "Epoch 334: New best model saved with Loss 19853.796875\n",
      "Epoch 335: New best model saved with Loss 19838.79296875\n",
      "Epoch 336: New best model saved with Loss 19823.84765625\n",
      "Epoch 337: New best model saved with Loss 19808.9609375\n",
      "Epoch 338: New best model saved with Loss 19794.1484375\n",
      "Epoch 339: New best model saved with Loss 19779.40234375\n",
      "Epoch 340: New best model saved with Loss 19764.66015625\n",
      "Epoch 341: New best model saved with Loss 19749.97265625\n",
      "Epoch 342: New best model saved with Loss 19735.369140625\n",
      "Epoch 343: New best model saved with Loss 19720.8359375\n",
      "Epoch 344: New best model saved with Loss 19706.240234375\n",
      "Epoch 345: New best model saved with Loss 19691.7890625\n",
      "Epoch 346: New best model saved with Loss 19677.326171875\n",
      "Epoch 347: New best model saved with Loss 19662.880859375\n",
      "Epoch 348: New best model saved with Loss 19648.4765625\n",
      "Epoch 349: New best model saved with Loss 19634.08203125\n",
      "Epoch 350: New best model saved with Loss 19619.7421875\n",
      "Epoch 351: New best model saved with Loss 19605.4375\n",
      "Epoch 352: New best model saved with Loss 19591.091796875\n",
      "Epoch 353: New best model saved with Loss 19576.859375\n",
      "Epoch 354: New best model saved with Loss 19562.64453125\n",
      "Epoch 355: New best model saved with Loss 19548.68359375\n",
      "Epoch 356: New best model saved with Loss 19534.8671875\n",
      "Epoch 357: New best model saved with Loss 19521.7578125\n",
      "Epoch 358: New best model saved with Loss 19509.912109375\n",
      "Epoch 359: New best model saved with Loss 19501.41796875\n",
      "Epoch 360: New best model saved with Loss 19500.806640625\n",
      "Epoch 418: New best model saved with Loss 19435.677734375\n",
      "Epoch 424: New best model saved with Loss 19274.81640625\n",
      "Epoch 427: New best model saved with Loss 19113.265625\n",
      "Epoch 430: New best model saved with Loss 19040.19921875\n",
      "Epoch 433: New best model saved with Loss 18991.673828125\n",
      "Epoch 434: New best model saved with Loss 18954.88671875\n",
      "Epoch 436: New best model saved with Loss 18885.4140625\n",
      "Epoch 437: New best model saved with Loss 18879.58984375\n",
      "Epoch 439: New best model saved with Loss 18778.546875\n",
      "Epoch 442: New best model saved with Loss 18709.203125\n",
      "Epoch 444: New best model saved with Loss 18695.67578125\n",
      "Epoch 445: New best model saved with Loss 18657.125\n",
      "Epoch 447: New best model saved with Loss 18623.52734375\n",
      "Epoch 448: New best model saved with Loss 18599.78125\n",
      "Epoch 449: New best model saved with Loss 18591.1171875\n",
      "Epoch 450: New best model saved with Loss 18560.34765625\n",
      "Epoch 451: New best model saved with Loss 18533.84375\n",
      "Epoch 452: New best model saved with Loss 18522.48046875\n",
      "Epoch 453: New best model saved with Loss 18496.15234375\n",
      "Epoch 454: New best model saved with Loss 18470.154296875\n",
      "Epoch 455: New best model saved with Loss 18460.95703125\n",
      "Epoch 456: New best model saved with Loss 18433.078125\n",
      "Epoch 457: New best model saved with Loss 18410.3125\n",
      "Epoch 458: New best model saved with Loss 18399.8828125\n",
      "Epoch 459: New best model saved with Loss 18369.607421875\n",
      "Epoch 460: New best model saved with Loss 18352.48828125\n",
      "Epoch 461: New best model saved with Loss 18337.40234375\n",
      "Epoch 462: New best model saved with Loss 18308.62890625\n",
      "Epoch 463: New best model saved with Loss 18295.1328125\n",
      "Epoch 464: New best model saved with Loss 18275.87109375\n",
      "Epoch 465: New best model saved with Loss 18250.859375\n",
      "Epoch 466: New best model saved with Loss 18237.48828125\n",
      "Epoch 467: New best model saved with Loss 18215.189453125\n",
      "Epoch 468: New best model saved with Loss 18193.71875\n",
      "Epoch 469: New best model saved with Loss 18178.583984375\n",
      "Epoch 470: New best model saved with Loss 18155.359375\n",
      "Epoch 471: New best model saved with Loss 18136.3984375\n",
      "Epoch 472: New best model saved with Loss 18119.21875\n",
      "Epoch 473: New best model saved with Loss 18096.693359375\n",
      "Epoch 474: New best model saved with Loss 18079.021484375\n",
      "Epoch 475: New best model saved with Loss 18060.046875\n",
      "Epoch 476: New best model saved with Loss 18038.44140625\n",
      "Epoch 477: New best model saved with Loss 18020.857421875\n",
      "Epoch 478: New best model saved with Loss 18000.63671875\n",
      "Epoch 479: New best model saved with Loss 17980.24609375\n",
      "Epoch 480: New best model saved with Loss 17962.00390625\n",
      "Epoch 481: New best model saved with Loss 17941.314453125\n",
      "Epoch 482: New best model saved with Loss 17921.521484375\n",
      "Epoch 483: New best model saved with Loss 17902.673828125\n",
      "Epoch 484: New best model saved with Loss 17881.904296875\n",
      "Epoch 485: New best model saved with Loss 17862.390625\n",
      "Epoch 486: New best model saved with Loss 17842.7890625\n",
      "Epoch 487: New best model saved with Loss 17822.1015625\n",
      "Epoch 488: New best model saved with Loss 17802.60546875\n",
      "Epoch 489: New best model saved with Loss 17782.396484375\n",
      "Epoch 490: New best model saved with Loss 17761.84765625\n",
      "Epoch 491: New best model saved with Loss 17742.0625\n",
      "Epoch 492: New best model saved with Loss 17721.5546875\n",
      "Epoch 493: New best model saved with Loss 17701.08984375\n",
      "Epoch 494: New best model saved with Loss 17680.947265625\n",
      "Epoch 495: New best model saved with Loss 17660.1484375\n",
      "Epoch 496: New best model saved with Loss 17639.564453125\n",
      "Epoch 497: New best model saved with Loss 17619.03515625\n",
      "Epoch 498: New best model saved with Loss 17598.0\n",
      "Epoch 499: New best model saved with Loss 17577.373046875\n",
      "Epoch 500: New best model saved with Loss 17556.4296875\n",
      "Epoch 501: New best model saved with Loss 17535.318359375\n",
      "Epoch 502: New best model saved with Loss 17514.38671875\n",
      "Epoch 503: New best model saved with Loss 17493.06640625\n",
      "Epoch 504: New best model saved with Loss 17471.728515625\n",
      "Epoch 505: New best model saved with Loss 17450.47265625\n",
      "Epoch 506: New best model saved with Loss 17428.904296875\n",
      "Epoch 507: New best model saved with Loss 17407.25\n",
      "Epoch 508: New best model saved with Loss 17385.57421875\n",
      "Epoch 509: New best model saved with Loss 17363.71875\n",
      "Epoch 510: New best model saved with Loss 17341.76953125\n",
      "Epoch 511: New best model saved with Loss 17319.677734375\n",
      "Epoch 512: New best model saved with Loss 17297.4375\n",
      "Epoch 513: New best model saved with Loss 17275.15234375\n",
      "Epoch 514: New best model saved with Loss 17252.728515625\n",
      "Epoch 515: New best model saved with Loss 17230.193359375\n",
      "Epoch 516: New best model saved with Loss 17207.53515625\n",
      "Epoch 517: New best model saved with Loss 17184.875\n",
      "Epoch 518: New best model saved with Loss 17162.080078125\n",
      "Epoch 519: New best model saved with Loss 17139.12890625\n",
      "Epoch 520: New best model saved with Loss 17116.140625\n",
      "Epoch 521: New best model saved with Loss 17093.1328125\n",
      "Epoch 522: New best model saved with Loss 17070.080078125\n",
      "Epoch 523: New best model saved with Loss 17046.8828125\n",
      "Epoch 524: New best model saved with Loss 17023.5703125\n",
      "Epoch 525: New best model saved with Loss 17000.154296875\n",
      "Epoch 526: New best model saved with Loss 16976.734375\n",
      "Epoch 527: New best model saved with Loss 16953.2265625\n",
      "Epoch 528: New best model saved with Loss 16929.533203125\n",
      "Epoch 529: New best model saved with Loss 16905.76953125\n",
      "Epoch 530: New best model saved with Loss 16881.921875\n",
      "Epoch 531: New best model saved with Loss 16857.998046875\n",
      "Epoch 532: New best model saved with Loss 16833.96484375\n",
      "Epoch 533: New best model saved with Loss 16809.865234375\n",
      "Epoch 534: New best model saved with Loss 16785.705078125\n",
      "Epoch 535: New best model saved with Loss 16761.46875\n",
      "Epoch 536: New best model saved with Loss 16737.244140625\n",
      "Epoch 537: New best model saved with Loss 16712.87109375\n",
      "Epoch 538: New best model saved with Loss 16688.40234375\n",
      "Epoch 539: New best model saved with Loss 16663.849609375\n",
      "Epoch 540: New best model saved with Loss 16639.28515625\n",
      "Epoch 541: New best model saved with Loss 16614.595703125\n",
      "Epoch 542: New best model saved with Loss 16589.830078125\n",
      "Epoch 543: New best model saved with Loss 16564.95703125\n",
      "Epoch 544: New best model saved with Loss 16540.056640625\n",
      "Epoch 545: New best model saved with Loss 16515.013671875\n",
      "Epoch 546: New best model saved with Loss 16489.9296875\n",
      "Epoch 547: New best model saved with Loss 16464.80859375\n",
      "Epoch 548: New best model saved with Loss 16439.453125\n",
      "Epoch 549: New best model saved with Loss 16414.1328125\n",
      "Epoch 550: New best model saved with Loss 16388.787109375\n",
      "Epoch 551: New best model saved with Loss 16363.162109375\n",
      "Epoch 552: New best model saved with Loss 16337.44921875\n",
      "Epoch 553: New best model saved with Loss 16311.73828125\n",
      "Epoch 554: New best model saved with Loss 16285.890625\n",
      "Epoch 555: New best model saved with Loss 16259.966796875\n",
      "Epoch 556: New best model saved with Loss 16233.986328125\n",
      "Epoch 557: New best model saved with Loss 16207.826171875\n",
      "Epoch 558: New best model saved with Loss 16181.548828125\n",
      "Epoch 559: New best model saved with Loss 16155.123046875\n",
      "Epoch 560: New best model saved with Loss 16128.716796875\n",
      "Epoch 561: New best model saved with Loss 16102.1640625\n",
      "Epoch 562: New best model saved with Loss 16075.482421875\n",
      "Epoch 563: New best model saved with Loss 16048.70703125\n",
      "Epoch 564: New best model saved with Loss 16021.8623046875\n",
      "Epoch 565: New best model saved with Loss 15994.970703125\n",
      "Epoch 566: New best model saved with Loss 15968.0\n",
      "Epoch 567: New best model saved with Loss 15940.98828125\n",
      "Epoch 568: New best model saved with Loss 15913.8701171875\n",
      "Epoch 569: New best model saved with Loss 15886.65625\n",
      "Epoch 570: New best model saved with Loss 15859.3515625\n",
      "Epoch 571: New best model saved with Loss 15832.056640625\n",
      "Epoch 572: New best model saved with Loss 15804.5576171875\n",
      "Epoch 573: New best model saved with Loss 15777.0703125\n",
      "Epoch 574: New best model saved with Loss 15749.484375\n",
      "Epoch 575: New best model saved with Loss 15721.7177734375\n",
      "Epoch 576: New best model saved with Loss 15693.9736328125\n",
      "Epoch 577: New best model saved with Loss 15666.005859375\n",
      "Epoch 578: New best model saved with Loss 15637.9375\n",
      "Epoch 579: New best model saved with Loss 15609.884765625\n",
      "Epoch 580: New best model saved with Loss 15581.5966796875\n",
      "Epoch 581: New best model saved with Loss 15553.267578125\n",
      "Epoch 582: New best model saved with Loss 15524.728515625\n",
      "Epoch 583: New best model saved with Loss 15496.033203125\n",
      "Epoch 584: New best model saved with Loss 15467.296875\n",
      "Epoch 585: New best model saved with Loss 15438.3095703125\n",
      "Epoch 586: New best model saved with Loss 15409.21875\n",
      "Epoch 587: New best model saved with Loss 15379.9921875\n",
      "Epoch 588: New best model saved with Loss 15350.6337890625\n",
      "Epoch 589: New best model saved with Loss 15321.181640625\n",
      "Epoch 590: New best model saved with Loss 15291.6171875\n",
      "Epoch 591: New best model saved with Loss 15262.080078125\n",
      "Epoch 592: New best model saved with Loss 15232.39453125\n",
      "Epoch 593: New best model saved with Loss 15202.587890625\n",
      "Epoch 594: New best model saved with Loss 15172.7333984375\n",
      "Epoch 595: New best model saved with Loss 15142.693359375\n",
      "Epoch 596: New best model saved with Loss 15112.580078125\n",
      "Epoch 597: New best model saved with Loss 15082.345703125\n",
      "Epoch 598: New best model saved with Loss 15052.001953125\n",
      "Epoch 599: New best model saved with Loss 15021.5009765625\n",
      "Epoch 600: New best model saved with Loss 14990.9140625\n",
      "Epoch 601: New best model saved with Loss 14960.138671875\n",
      "Epoch 602: New best model saved with Loss 14929.45703125\n",
      "Epoch 603: New best model saved with Loss 14898.4638671875\n",
      "Epoch 604: New best model saved with Loss 14867.4375\n",
      "Epoch 605: New best model saved with Loss 14836.494140625\n",
      "Epoch 606: New best model saved with Loss 14805.2509765625\n",
      "Epoch 607: New best model saved with Loss 14774.115234375\n",
      "Epoch 608: New best model saved with Loss 14742.625\n",
      "Epoch 609: New best model saved with Loss 14711.142578125\n",
      "Epoch 610: New best model saved with Loss 14679.541015625\n",
      "Epoch 611: New best model saved with Loss 14647.7841796875\n",
      "Epoch 612: New best model saved with Loss 14615.8359375\n",
      "Epoch 613: New best model saved with Loss 14583.9287109375\n",
      "Epoch 614: New best model saved with Loss 14551.837890625\n",
      "Epoch 615: New best model saved with Loss 14519.529296875\n",
      "Epoch 616: New best model saved with Loss 14487.080078125\n",
      "Epoch 617: New best model saved with Loss 14454.4404296875\n",
      "Epoch 618: New best model saved with Loss 14421.6875\n",
      "Epoch 619: New best model saved with Loss 14388.73046875\n",
      "Epoch 620: New best model saved with Loss 14355.60546875\n",
      "Epoch 621: New best model saved with Loss 14322.396484375\n",
      "Epoch 622: New best model saved with Loss 14288.7939453125\n",
      "Epoch 623: New best model saved with Loss 14255.091796875\n",
      "Epoch 624: New best model saved with Loss 14221.248046875\n",
      "Epoch 625: New best model saved with Loss 14187.291015625\n",
      "Epoch 626: New best model saved with Loss 14153.140625\n",
      "Epoch 627: New best model saved with Loss 14118.8095703125\n",
      "Epoch 628: New best model saved with Loss 14084.4716796875\n",
      "Epoch 629: New best model saved with Loss 14050.02734375\n",
      "Epoch 630: New best model saved with Loss 14015.6474609375\n",
      "Epoch 631: New best model saved with Loss 13981.234375\n",
      "Epoch 632: New best model saved with Loss 13946.5654296875\n",
      "Epoch 633: New best model saved with Loss 13911.8662109375\n",
      "Epoch 634: New best model saved with Loss 13877.1015625\n",
      "Epoch 635: New best model saved with Loss 13842.1015625\n",
      "Epoch 636: New best model saved with Loss 13806.95703125\n",
      "Epoch 637: New best model saved with Loss 13771.767578125\n",
      "Epoch 638: New best model saved with Loss 13736.361328125\n",
      "Epoch 639: New best model saved with Loss 13701.009765625\n",
      "Epoch 640: New best model saved with Loss 13665.541015625\n",
      "Epoch 641: New best model saved with Loss 13629.9365234375\n",
      "Epoch 642: New best model saved with Loss 13594.1923828125\n",
      "Epoch 643: New best model saved with Loss 13558.30078125\n",
      "Epoch 644: New best model saved with Loss 13522.564453125\n",
      "Epoch 645: New best model saved with Loss 13486.80859375\n",
      "Epoch 646: New best model saved with Loss 13451.138671875\n",
      "Epoch 647: New best model saved with Loss 13415.193359375\n",
      "Epoch 648: New best model saved with Loss 13379.306640625\n",
      "Epoch 649: New best model saved with Loss 13343.5361328125\n",
      "Epoch 650: New best model saved with Loss 13307.96875\n",
      "Epoch 651: New best model saved with Loss 13272.302734375\n",
      "Epoch 652: New best model saved with Loss 13236.521484375\n",
      "Epoch 653: New best model saved with Loss 13200.9609375\n",
      "Epoch 654: New best model saved with Loss 13165.2646484375\n",
      "Epoch 655: New best model saved with Loss 13129.61328125\n",
      "Epoch 656: New best model saved with Loss 13093.875\n",
      "Epoch 657: New best model saved with Loss 13058.033203125\n",
      "Epoch 658: New best model saved with Loss 13022.06640625\n",
      "Epoch 659: New best model saved with Loss 12986.1689453125\n",
      "Epoch 660: New best model saved with Loss 12950.291015625\n",
      "Epoch 661: New best model saved with Loss 12913.9638671875\n",
      "Epoch 662: New best model saved with Loss 12877.87890625\n",
      "Epoch 663: New best model saved with Loss 12841.6875\n",
      "Epoch 664: New best model saved with Loss 12805.47265625\n",
      "Epoch 665: New best model saved with Loss 12769.0751953125\n",
      "Epoch 666: New best model saved with Loss 12732.62890625\n",
      "Epoch 667: New best model saved with Loss 12696.1953125\n",
      "Epoch 668: New best model saved with Loss 12659.62890625\n",
      "Epoch 669: New best model saved with Loss 12622.916015625\n",
      "Epoch 670: New best model saved with Loss 12586.013671875\n",
      "Epoch 671: New best model saved with Loss 12549.099609375\n",
      "Epoch 672: New best model saved with Loss 12512.1484375\n",
      "Epoch 673: New best model saved with Loss 12475.01953125\n",
      "Epoch 674: New best model saved with Loss 12438.0654296875\n",
      "Epoch 675: New best model saved with Loss 12400.96875\n",
      "Epoch 676: New best model saved with Loss 12363.564453125\n",
      "Epoch 677: New best model saved with Loss 12326.1962890625\n",
      "Epoch 678: New best model saved with Loss 12288.666015625\n",
      "Epoch 679: New best model saved with Loss 12251.009765625\n",
      "Epoch 680: New best model saved with Loss 12213.1875\n",
      "Epoch 681: New best model saved with Loss 12175.390625\n",
      "Epoch 682: New best model saved with Loss 12137.529296875\n",
      "Epoch 683: New best model saved with Loss 12099.6875\n",
      "Epoch 684: New best model saved with Loss 12062.009765625\n",
      "Epoch 685: New best model saved with Loss 12024.31640625\n",
      "Epoch 686: New best model saved with Loss 11987.068359375\n",
      "Epoch 687: New best model saved with Loss 11951.0625\n",
      "Epoch 688: New best model saved with Loss 11917.591796875\n",
      "Epoch 689: New best model saved with Loss 11889.5859375\n",
      "Epoch 690: New best model saved with Loss 11874.3837890625\n",
      "Epoch 736: New best model saved with Loss 11577.9267578125\n",
      "Epoch 739: New best model saved with Loss 11428.35546875\n",
      "Epoch 741: New best model saved with Loss 10911.6240234375\n",
      "Epoch 744: New best model saved with Loss 10484.5751953125\n",
      "Epoch 747: New best model saved with Loss 10362.9873046875\n",
      "Epoch 749: New best model saved with Loss 10210.212890625\n",
      "Epoch 752: New best model saved with Loss 10067.6953125\n",
      "Epoch 755: New best model saved with Loss 9929.7138671875\n",
      "Epoch 757: New best model saved with Loss 9815.7158203125\n",
      "Epoch 758: New best model saved with Loss 9791.96875\n",
      "Epoch 759: New best model saved with Loss 9778.45703125\n",
      "Epoch 760: New best model saved with Loss 9596.0634765625\n",
      "Epoch 762: New best model saved with Loss 9547.5546875\n",
      "Epoch 763: New best model saved with Loss 9426.76953125\n",
      "Epoch 764: New best model saved with Loss 9400.9208984375\n",
      "Epoch 765: New best model saved with Loss 9340.560546875\n",
      "Epoch 766: New best model saved with Loss 9237.2685546875\n",
      "Epoch 767: New best model saved with Loss 9215.1201171875\n",
      "Epoch 768: New best model saved with Loss 9143.513671875\n",
      "Epoch 769: New best model saved with Loss 9059.32421875\n",
      "Epoch 770: New best model saved with Loss 9026.189453125\n",
      "Epoch 771: New best model saved with Loss 8947.5322265625\n",
      "Epoch 772: New best model saved with Loss 8893.80078125\n",
      "Epoch 773: New best model saved with Loss 8838.00390625\n",
      "Epoch 774: New best model saved with Loss 8768.55859375\n",
      "Epoch 775: New best model saved with Loss 8724.134765625\n",
      "Epoch 776: New best model saved with Loss 8650.2666015625\n",
      "Epoch 777: New best model saved with Loss 8597.48828125\n",
      "Epoch 778: New best model saved with Loss 8550.9169921875\n",
      "Epoch 779: New best model saved with Loss 8471.3779296875\n",
      "Epoch 780: New best model saved with Loss 8429.1337890625\n",
      "Epoch 781: New best model saved with Loss 8368.728515625\n",
      "Epoch 782: New best model saved with Loss 8291.705078125\n",
      "Epoch 783: New best model saved with Loss 8252.4111328125\n",
      "Epoch 784: New best model saved with Loss 8178.228515625\n",
      "Epoch 785: New best model saved with Loss 8115.0703125\n",
      "Epoch 786: New best model saved with Loss 8067.97021484375\n",
      "Epoch 787: New best model saved with Loss 7988.58740234375\n",
      "Epoch 788: New best model saved with Loss 7934.26171875\n",
      "Epoch 789: New best model saved with Loss 7873.1982421875\n",
      "Epoch 790: New best model saved with Loss 7801.2255859375\n",
      "Epoch 791: New best model saved with Loss 7743.52294921875\n",
      "Epoch 792: New best model saved with Loss 7676.53759765625\n",
      "Epoch 793: New best model saved with Loss 7614.439453125\n",
      "Epoch 794: New best model saved with Loss 7547.92724609375\n",
      "Epoch 795: New best model saved with Loss 7485.53857421875\n",
      "Epoch 796: New best model saved with Loss 7424.041015625\n",
      "Epoch 797: New best model saved with Loss 7353.50927734375\n",
      "Epoch 798: New best model saved with Loss 7294.09521484375\n",
      "Epoch 799: New best model saved with Loss 7230.48974609375\n",
      "Epoch 800: New best model saved with Loss 7164.845703125\n",
      "Epoch 801: New best model saved with Loss 7104.1611328125\n",
      "Epoch 802: New best model saved with Loss 7039.8251953125\n",
      "Epoch 803: New best model saved with Loss 6976.86279296875\n",
      "Epoch 804: New best model saved with Loss 6911.5703125\n",
      "Epoch 805: New best model saved with Loss 6848.1064453125\n",
      "Epoch 806: New best model saved with Loss 6784.576171875\n",
      "Epoch 807: New best model saved with Loss 6717.0703125\n",
      "Epoch 808: New best model saved with Loss 6652.861328125\n",
      "Epoch 809: New best model saved with Loss 6587.0400390625\n",
      "Epoch 810: New best model saved with Loss 6520.0185546875\n",
      "Epoch 811: New best model saved with Loss 6455.0556640625\n",
      "Epoch 812: New best model saved with Loss 6390.25341796875\n",
      "Epoch 813: New best model saved with Loss 6325.9697265625\n",
      "Epoch 814: New best model saved with Loss 6260.013671875\n",
      "Epoch 815: New best model saved with Loss 6194.666015625\n",
      "Epoch 816: New best model saved with Loss 6130.08203125\n",
      "Epoch 817: New best model saved with Loss 6064.673828125\n",
      "Epoch 818: New best model saved with Loss 5999.39599609375\n",
      "Epoch 819: New best model saved with Loss 5932.8408203125\n",
      "Epoch 820: New best model saved with Loss 5867.248046875\n",
      "Epoch 821: New best model saved with Loss 5800.75\n",
      "Epoch 822: New best model saved with Loss 5734.19580078125\n",
      "Epoch 823: New best model saved with Loss 5667.7841796875\n",
      "Epoch 824: New best model saved with Loss 5601.044921875\n",
      "Epoch 825: New best model saved with Loss 5534.67919921875\n",
      "Epoch 826: New best model saved with Loss 5468.6455078125\n",
      "Epoch 827: New best model saved with Loss 5402.92138671875\n",
      "Epoch 828: New best model saved with Loss 5337.365234375\n",
      "Epoch 829: New best model saved with Loss 5272.123046875\n",
      "Epoch 830: New best model saved with Loss 5206.5126953125\n",
      "Epoch 831: New best model saved with Loss 5141.056640625\n",
      "Epoch 832: New best model saved with Loss 5075.666015625\n",
      "Epoch 833: New best model saved with Loss 5010.90380859375\n",
      "Epoch 834: New best model saved with Loss 4946.138671875\n",
      "Epoch 835: New best model saved with Loss 4881.52392578125\n",
      "Epoch 836: New best model saved with Loss 4816.8125\n",
      "Epoch 837: New best model saved with Loss 4752.728515625\n",
      "Epoch 838: New best model saved with Loss 4688.26953125\n",
      "Epoch 839: New best model saved with Loss 4624.5126953125\n",
      "Epoch 840: New best model saved with Loss 4560.93359375\n",
      "Epoch 841: New best model saved with Loss 4497.51416015625\n",
      "Epoch 842: New best model saved with Loss 4434.423828125\n",
      "Epoch 843: New best model saved with Loss 4371.142578125\n",
      "Epoch 844: New best model saved with Loss 4308.59375\n",
      "Epoch 845: New best model saved with Loss 4246.32275390625\n",
      "Epoch 846: New best model saved with Loss 4184.8095703125\n",
      "Epoch 847: New best model saved with Loss 4123.4619140625\n",
      "Epoch 848: New best model saved with Loss 4062.77734375\n",
      "Epoch 849: New best model saved with Loss 4002.6953125\n",
      "Epoch 850: New best model saved with Loss 3943.8515625\n",
      "Epoch 851: New best model saved with Loss 3885.103271484375\n",
      "Epoch 852: New best model saved with Loss 3827.329833984375\n",
      "Epoch 853: New best model saved with Loss 3769.572998046875\n",
      "Epoch 854: New best model saved with Loss 3712.509521484375\n",
      "Epoch 855: New best model saved with Loss 3655.8056640625\n",
      "Epoch 856: New best model saved with Loss 3599.67578125\n",
      "Epoch 857: New best model saved with Loss 3544.18701171875\n",
      "Epoch 858: New best model saved with Loss 3489.38818359375\n",
      "Epoch 859: New best model saved with Loss 3435.237548828125\n",
      "Epoch 860: New best model saved with Loss 3381.875244140625\n",
      "Epoch 861: New best model saved with Loss 3329.250732421875\n",
      "Epoch 862: New best model saved with Loss 3277.56884765625\n",
      "Epoch 863: New best model saved with Loss 3226.487548828125\n",
      "Epoch 864: New best model saved with Loss 3176.3251953125\n",
      "Epoch 865: New best model saved with Loss 3127.10791015625\n",
      "Epoch 866: New best model saved with Loss 3078.6923828125\n",
      "Epoch 867: New best model saved with Loss 3031.085693359375\n",
      "Epoch 868: New best model saved with Loss 2984.331787109375\n",
      "Epoch 869: New best model saved with Loss 2938.4814453125\n",
      "Epoch 870: New best model saved with Loss 2892.658203125\n",
      "Epoch 871: New best model saved with Loss 2847.018798828125\n",
      "Epoch 872: New best model saved with Loss 2801.812255859375\n",
      "Epoch 873: New best model saved with Loss 2757.467529296875\n",
      "Epoch 874: New best model saved with Loss 2713.3486328125\n",
      "Epoch 875: New best model saved with Loss 2670.38232421875\n",
      "Epoch 876: New best model saved with Loss 2628.45361328125\n",
      "Epoch 877: New best model saved with Loss 2587.8544921875\n",
      "Epoch 878: New best model saved with Loss 2548.46484375\n",
      "Epoch 879: New best model saved with Loss 2510.45947265625\n",
      "Epoch 880: New best model saved with Loss 2473.126953125\n",
      "Epoch 881: New best model saved with Loss 2436.5390625\n",
      "Epoch 882: New best model saved with Loss 2401.15673828125\n",
      "Epoch 883: New best model saved with Loss 2366.62841796875\n",
      "Epoch 884: New best model saved with Loss 2333.41796875\n",
      "Epoch 885: New best model saved with Loss 2301.37646484375\n",
      "Epoch 886: New best model saved with Loss 2270.95654296875\n",
      "Epoch 887: New best model saved with Loss 2242.9814453125\n",
      "Epoch 888: New best model saved with Loss 2220.32373046875\n",
      "Epoch 889: New best model saved with Loss 2209.474853515625\n",
      "Epoch 934: New best model saved with Loss 1992.4285888671875\n",
      "Epoch 937: New best model saved with Loss 1942.635498046875\n",
      "Epoch 939: New best model saved with Loss 1725.584716796875\n",
      "Epoch 942: New best model saved with Loss 1659.7313232421875\n",
      "Epoch 944: New best model saved with Loss 1584.080810546875\n",
      "Epoch 947: New best model saved with Loss 1515.767333984375\n",
      "Epoch 950: New best model saved with Loss 1488.075927734375\n",
      "Epoch 952: New best model saved with Loss 1470.866455078125\n",
      "Epoch 953: New best model saved with Loss 1438.2257080078125\n",
      "Epoch 955: New best model saved with Loss 1396.455810546875\n",
      "Epoch 958: New best model saved with Loss 1351.81494140625\n",
      "Epoch 961: New best model saved with Loss 1326.8251953125\n",
      "Epoch 962: New best model saved with Loss 1311.491455078125\n",
      "Epoch 964: New best model saved with Loss 1288.4508056640625\n",
      "Epoch 965: New best model saved with Loss 1281.4915771484375\n",
      "Epoch 967: New best model saved with Loss 1257.29248046875\n",
      "Epoch 968: New best model saved with Loss 1252.491455078125\n",
      "Epoch 970: New best model saved with Loss 1233.53759765625\n",
      "Epoch 971: New best model saved with Loss 1225.196533203125\n",
      "Epoch 972: New best model saved with Loss 1224.4012451171875\n",
      "Epoch 973: New best model saved with Loss 1211.8106689453125\n",
      "Epoch 974: New best model saved with Loss 1197.4149169921875\n",
      "Epoch 976: New best model saved with Loss 1188.8865966796875\n",
      "Epoch 977: New best model saved with Loss 1174.9144287109375\n",
      "Epoch 979: New best model saved with Loss 1166.015625\n",
      "Epoch 980: New best model saved with Loss 1156.4945068359375\n",
      "Epoch 981: New best model saved with Loss 1151.140380859375\n",
      "Epoch 982: New best model saved with Loss 1147.134033203125\n",
      "Epoch 983: New best model saved with Loss 1136.7823486328125\n",
      "Epoch 984: New best model saved with Loss 1130.6021728515625\n",
      "Epoch 985: New best model saved with Loss 1127.240234375\n",
      "Epoch 986: New best model saved with Loss 1118.2275390625\n",
      "Epoch 987: New best model saved with Loss 1112.3424072265625\n",
      "Epoch 988: New best model saved with Loss 1108.035400390625\n",
      "Epoch 989: New best model saved with Loss 1101.4639892578125\n",
      "Epoch 990: New best model saved with Loss 1094.746337890625\n",
      "Epoch 991: New best model saved with Loss 1089.98681640625\n",
      "Epoch 992: New best model saved with Loss 1085.123046875\n",
      "Epoch 993: New best model saved with Loss 1077.78857421875\n",
      "Epoch 994: New best model saved with Loss 1073.196533203125\n",
      "Epoch 995: New best model saved with Loss 1068.118896484375\n",
      "Epoch 996: New best model saved with Loss 1062.1343994140625\n",
      "Epoch 997: New best model saved with Loss 1056.6519775390625\n",
      "Epoch 998: New best model saved with Loss 1052.11279296875\n",
      "Epoch 999: New best model saved with Loss 1046.9471435546875\n",
      "Epoch 1000: New best model saved with Loss 1041.1995849609375\n",
      "Epoch 1001: New best model saved with Loss 1036.7830810546875\n",
      "Epoch 1002: New best model saved with Loss 1031.951171875\n",
      "Epoch 1003: New best model saved with Loss 1026.6072998046875\n",
      "Epoch 1004: New best model saved with Loss 1021.8338623046875\n",
      "Epoch 1005: New best model saved with Loss 1017.3145141601562\n",
      "Epoch 1006: New best model saved with Loss 1012.4431762695312\n",
      "Epoch 1007: New best model saved with Loss 1007.4196166992188\n",
      "Epoch 1008: New best model saved with Loss 1003.0431518554688\n",
      "Epoch 1009: New best model saved with Loss 998.500732421875\n",
      "Epoch 1010: New best model saved with Loss 993.6442260742188\n",
      "Epoch 1011: New best model saved with Loss 989.169921875\n",
      "Epoch 1012: New best model saved with Loss 984.8291015625\n",
      "Epoch 1013: New best model saved with Loss 980.3865356445312\n",
      "Epoch 1014: New best model saved with Loss 975.8690185546875\n",
      "Epoch 1015: New best model saved with Loss 971.650634765625\n",
      "Epoch 1016: New best model saved with Loss 967.38525390625\n",
      "Epoch 1017: New best model saved with Loss 963.0244750976562\n",
      "Epoch 1018: New best model saved with Loss 958.7525024414062\n",
      "Epoch 1019: New best model saved with Loss 954.6267700195312\n",
      "Epoch 1020: New best model saved with Loss 950.4766845703125\n",
      "Epoch 1021: New best model saved with Loss 946.2587890625\n",
      "Epoch 1022: New best model saved with Loss 942.218994140625\n",
      "Epoch 1023: New best model saved with Loss 938.1802978515625\n",
      "Epoch 1024: New best model saved with Loss 934.1582641601562\n",
      "Epoch 1025: New best model saved with Loss 930.1323852539062\n",
      "Epoch 1026: New best model saved with Loss 926.2444458007812\n",
      "Epoch 1027: New best model saved with Loss 922.3912963867188\n",
      "Epoch 1028: New best model saved with Loss 918.47998046875\n",
      "Epoch 1029: New best model saved with Loss 914.6864013671875\n",
      "Epoch 1030: New best model saved with Loss 910.84814453125\n",
      "Epoch 1031: New best model saved with Loss 907.0205688476562\n",
      "Epoch 1032: New best model saved with Loss 903.2460327148438\n",
      "Epoch 1033: New best model saved with Loss 899.4346313476562\n",
      "Epoch 1034: New best model saved with Loss 895.749755859375\n",
      "Epoch 1035: New best model saved with Loss 892.0480346679688\n",
      "Epoch 1036: New best model saved with Loss 888.4275512695312\n",
      "Epoch 1037: New best model saved with Loss 884.8179321289062\n",
      "Epoch 1038: New best model saved with Loss 881.289306640625\n",
      "Epoch 1039: New best model saved with Loss 877.7937622070312\n",
      "Epoch 1040: New best model saved with Loss 874.3917846679688\n",
      "Epoch 1041: New best model saved with Loss 870.968017578125\n",
      "Epoch 1042: New best model saved with Loss 867.5977783203125\n",
      "Epoch 1043: New best model saved with Loss 864.3367919921875\n",
      "Epoch 1044: New best model saved with Loss 861.0445556640625\n",
      "Epoch 1045: New best model saved with Loss 857.854736328125\n",
      "Epoch 1046: New best model saved with Loss 854.642333984375\n",
      "Epoch 1047: New best model saved with Loss 851.478515625\n",
      "Epoch 1048: New best model saved with Loss 848.3157348632812\n",
      "Epoch 1049: New best model saved with Loss 845.2174072265625\n",
      "Epoch 1050: New best model saved with Loss 842.1105346679688\n",
      "Epoch 1051: New best model saved with Loss 839.0867309570312\n",
      "Epoch 1052: New best model saved with Loss 836.0321044921875\n",
      "Epoch 1053: New best model saved with Loss 833.0352783203125\n",
      "Epoch 1054: New best model saved with Loss 830.0540161132812\n",
      "Epoch 1055: New best model saved with Loss 827.0784301757812\n",
      "Epoch 1056: New best model saved with Loss 824.1563110351562\n",
      "Epoch 1057: New best model saved with Loss 821.2978515625\n",
      "Epoch 1058: New best model saved with Loss 818.4642944335938\n",
      "Epoch 1059: New best model saved with Loss 815.6553955078125\n",
      "Epoch 1060: New best model saved with Loss 812.818359375\n",
      "Epoch 1061: New best model saved with Loss 810.0372314453125\n",
      "Epoch 1062: New best model saved with Loss 807.2874755859375\n",
      "Epoch 1063: New best model saved with Loss 804.5535278320312\n",
      "Epoch 1064: New best model saved with Loss 801.8368530273438\n",
      "Epoch 1065: New best model saved with Loss 799.1312255859375\n",
      "Epoch 1066: New best model saved with Loss 796.47900390625\n",
      "Epoch 1067: New best model saved with Loss 793.8199462890625\n",
      "Epoch 1068: New best model saved with Loss 791.1768188476562\n",
      "Epoch 1069: New best model saved with Loss 788.5602416992188\n",
      "Epoch 1070: New best model saved with Loss 785.9481811523438\n",
      "Epoch 1071: New best model saved with Loss 783.35546875\n",
      "Epoch 1072: New best model saved with Loss 780.75439453125\n",
      "Epoch 1073: New best model saved with Loss 778.178466796875\n",
      "Epoch 1074: New best model saved with Loss 775.6124877929688\n",
      "Epoch 1075: New best model saved with Loss 773.0545654296875\n",
      "Epoch 1076: New best model saved with Loss 770.5162963867188\n",
      "Epoch 1077: New best model saved with Loss 767.980712890625\n",
      "Epoch 1078: New best model saved with Loss 765.4839477539062\n",
      "Epoch 1079: New best model saved with Loss 763.0149536132812\n",
      "Epoch 1080: New best model saved with Loss 760.53662109375\n",
      "Epoch 1081: New best model saved with Loss 758.0928955078125\n",
      "Epoch 1082: New best model saved with Loss 755.6491088867188\n",
      "Epoch 1083: New best model saved with Loss 753.2532958984375\n",
      "Epoch 1084: New best model saved with Loss 750.8548583984375\n",
      "Epoch 1085: New best model saved with Loss 748.462646484375\n",
      "Epoch 1086: New best model saved with Loss 746.0726318359375\n",
      "Epoch 1087: New best model saved with Loss 743.6722412109375\n",
      "Epoch 1088: New best model saved with Loss 741.356201171875\n",
      "Epoch 1089: New best model saved with Loss 738.9931640625\n",
      "Epoch 1090: New best model saved with Loss 736.6437377929688\n",
      "Epoch 1091: New best model saved with Loss 734.3021850585938\n",
      "Epoch 1092: New best model saved with Loss 731.9524536132812\n",
      "Epoch 1093: New best model saved with Loss 729.6814575195312\n",
      "Epoch 1094: New best model saved with Loss 727.3906860351562\n",
      "Epoch 1095: New best model saved with Loss 725.15087890625\n",
      "Epoch 1096: New best model saved with Loss 722.8822021484375\n",
      "Epoch 1097: New best model saved with Loss 720.609130859375\n",
      "Epoch 1098: New best model saved with Loss 718.39404296875\n",
      "Epoch 1099: New best model saved with Loss 716.1679077148438\n",
      "Epoch 1100: New best model saved with Loss 713.9705810546875\n",
      "Epoch 1101: New best model saved with Loss 711.7763671875\n",
      "Epoch 1102: New best model saved with Loss 709.6058349609375\n",
      "Epoch 1103: New best model saved with Loss 707.4407958984375\n",
      "Epoch 1104: New best model saved with Loss 705.2739868164062\n",
      "Epoch 1105: New best model saved with Loss 703.1605834960938\n",
      "Epoch 1106: New best model saved with Loss 701.0279541015625\n",
      "Epoch 1107: New best model saved with Loss 698.9237060546875\n",
      "Epoch 1108: New best model saved with Loss 696.8233032226562\n",
      "Epoch 1109: New best model saved with Loss 694.6940307617188\n",
      "Epoch 1110: New best model saved with Loss 692.6615600585938\n",
      "Epoch 1111: New best model saved with Loss 690.5760498046875\n",
      "Epoch 1112: New best model saved with Loss 688.5014038085938\n",
      "Epoch 1113: New best model saved with Loss 686.45068359375\n",
      "Epoch 1114: New best model saved with Loss 684.3956909179688\n",
      "Epoch 1115: New best model saved with Loss 682.3434448242188\n",
      "Epoch 1116: New best model saved with Loss 680.298095703125\n",
      "Epoch 1117: New best model saved with Loss 678.2742919921875\n",
      "Epoch 1118: New best model saved with Loss 676.212158203125\n",
      "Epoch 1119: New best model saved with Loss 674.17822265625\n",
      "Epoch 1120: New best model saved with Loss 672.1575927734375\n",
      "Epoch 1121: New best model saved with Loss 670.1177368164062\n",
      "Epoch 1122: New best model saved with Loss 668.0814819335938\n",
      "Epoch 1123: New best model saved with Loss 666.0843505859375\n",
      "Epoch 1124: New best model saved with Loss 664.0784912109375\n",
      "Epoch 1125: New best model saved with Loss 662.1005859375\n",
      "Epoch 1126: New best model saved with Loss 660.1331176757812\n",
      "Epoch 1127: New best model saved with Loss 658.1831665039062\n",
      "Epoch 1128: New best model saved with Loss 656.226806640625\n",
      "Epoch 1129: New best model saved with Loss 654.29443359375\n",
      "Epoch 1130: New best model saved with Loss 652.3663940429688\n",
      "Epoch 1131: New best model saved with Loss 650.43701171875\n",
      "Epoch 1132: New best model saved with Loss 648.5133056640625\n",
      "Epoch 1133: New best model saved with Loss 646.6057739257812\n",
      "Epoch 1134: New best model saved with Loss 644.689453125\n",
      "Epoch 1135: New best model saved with Loss 642.767578125\n",
      "Epoch 1136: New best model saved with Loss 640.8811645507812\n",
      "Epoch 1137: New best model saved with Loss 638.998779296875\n",
      "Epoch 1138: New best model saved with Loss 637.1123046875\n",
      "Epoch 1139: New best model saved with Loss 635.2399291992188\n",
      "Epoch 1140: New best model saved with Loss 633.3792114257812\n",
      "Epoch 1141: New best model saved with Loss 631.5347900390625\n",
      "Epoch 1142: New best model saved with Loss 629.6942138671875\n",
      "Epoch 1143: New best model saved with Loss 627.8828125\n",
      "Epoch 1144: New best model saved with Loss 626.0779418945312\n",
      "Epoch 1145: New best model saved with Loss 624.282470703125\n",
      "Epoch 1146: New best model saved with Loss 622.4887084960938\n",
      "Epoch 1147: New best model saved with Loss 620.7224731445312\n",
      "Epoch 1148: New best model saved with Loss 618.9235229492188\n",
      "Epoch 1149: New best model saved with Loss 617.1448974609375\n",
      "Epoch 1150: New best model saved with Loss 615.402587890625\n",
      "Epoch 1151: New best model saved with Loss 613.6356811523438\n",
      "Epoch 1152: New best model saved with Loss 611.8822631835938\n",
      "Epoch 1153: New best model saved with Loss 610.1221923828125\n",
      "Epoch 1154: New best model saved with Loss 608.4044799804688\n",
      "Epoch 1155: New best model saved with Loss 606.6981811523438\n",
      "Epoch 1156: New best model saved with Loss 605.0687255859375\n",
      "Epoch 1157: New best model saved with Loss 603.5012817382812\n",
      "Epoch 1158: New best model saved with Loss 602.0830078125\n",
      "Epoch 1159: New best model saved with Loss 600.8997802734375\n",
      "Epoch 1160: New best model saved with Loss 600.1883544921875\n",
      "Epoch 1212: New best model saved with Loss 597.5903930664062\n",
      "Epoch 1216: New best model saved with Loss 580.1722412109375\n",
      "Epoch 1220: New best model saved with Loss 563.9724731445312\n",
      "Epoch 1224: New best model saved with Loss 557.6248168945312\n",
      "Epoch 1225: New best model saved with Loss 552.5408935546875\n",
      "Epoch 1229: New best model saved with Loss 541.047119140625\n",
      "Epoch 1233: New best model saved with Loss 537.9029541015625\n",
      "Epoch 1234: New best model saved with Loss 530.6890869140625\n",
      "Epoch 1238: New best model saved with Loss 527.2783813476562\n",
      "Epoch 1239: New best model saved with Loss 521.917236328125\n",
      "Epoch 1243: New best model saved with Loss 518.93505859375\n",
      "Epoch 1244: New best model saved with Loss 514.1397705078125\n",
      "Epoch 1245: New best model saved with Loss 512.3153076171875\n",
      "Epoch 1248: New best model saved with Loss 510.9210205078125\n",
      "Epoch 1249: New best model saved with Loss 507.4654235839844\n",
      "Epoch 1250: New best model saved with Loss 504.4150695800781\n",
      "Epoch 1251: New best model saved with Loss 502.80487060546875\n",
      "Epoch 1252: New best model saved with Loss 502.42095947265625\n",
      "Epoch 1253: New best model saved with Loss 501.9310302734375\n",
      "Epoch 1254: New best model saved with Loss 500.54144287109375\n",
      "Epoch 1255: New best model saved with Loss 498.2745361328125\n",
      "Epoch 1256: New best model saved with Loss 495.86669921875\n",
      "Epoch 1257: New best model saved with Loss 493.9952087402344\n",
      "Epoch 1258: New best model saved with Loss 492.8280334472656\n",
      "Epoch 1259: New best model saved with Loss 492.060791015625\n",
      "Epoch 1260: New best model saved with Loss 491.125\n",
      "Epoch 1261: New best model saved with Loss 489.823486328125\n",
      "Epoch 1262: New best model saved with Loss 488.1307373046875\n",
      "Epoch 1263: New best model saved with Loss 486.31915283203125\n",
      "Epoch 1264: New best model saved with Loss 484.6658935546875\n",
      "Epoch 1265: New best model saved with Loss 483.2537841796875\n",
      "Epoch 1266: New best model saved with Loss 482.0858154296875\n",
      "Epoch 1267: New best model saved with Loss 481.02099609375\n",
      "Epoch 1268: New best model saved with Loss 479.93255615234375\n",
      "Epoch 1269: New best model saved with Loss 478.71856689453125\n",
      "Epoch 1270: New best model saved with Loss 477.37957763671875\n",
      "Epoch 1271: New best model saved with Loss 475.9532165527344\n",
      "Epoch 1272: New best model saved with Loss 474.544189453125\n",
      "Epoch 1273: New best model saved with Loss 473.18670654296875\n",
      "Epoch 1274: New best model saved with Loss 471.8697814941406\n",
      "Epoch 1275: New best model saved with Loss 470.6583251953125\n",
      "Epoch 1276: New best model saved with Loss 469.50115966796875\n",
      "Epoch 1277: New best model saved with Loss 468.36761474609375\n",
      "Epoch 1278: New best model saved with Loss 467.26397705078125\n",
      "Epoch 1279: New best model saved with Loss 466.1387634277344\n",
      "Epoch 1280: New best model saved with Loss 465.017578125\n",
      "Epoch 1281: New best model saved with Loss 463.8721923828125\n",
      "Epoch 1282: New best model saved with Loss 462.7162170410156\n",
      "Epoch 1283: New best model saved with Loss 461.5547790527344\n",
      "Epoch 1284: New best model saved with Loss 460.386962890625\n",
      "Epoch 1285: New best model saved with Loss 459.2022705078125\n",
      "Epoch 1286: New best model saved with Loss 458.0311279296875\n",
      "Epoch 1287: New best model saved with Loss 456.8965148925781\n",
      "Epoch 1288: New best model saved with Loss 455.7397155761719\n",
      "Epoch 1289: New best model saved with Loss 454.61962890625\n",
      "Epoch 1290: New best model saved with Loss 453.5293273925781\n",
      "Epoch 1291: New best model saved with Loss 452.48095703125\n",
      "Epoch 1292: New best model saved with Loss 451.464599609375\n",
      "Epoch 1293: New best model saved with Loss 450.5405578613281\n",
      "Epoch 1294: New best model saved with Loss 449.690673828125\n",
      "Epoch 1295: New best model saved with Loss 448.9776916503906\n",
      "Epoch 1296: New best model saved with Loss 448.4971923828125\n",
      "Epoch 1297: New best model saved with Loss 448.4129638671875\n",
      "Epoch 1352: New best model saved with Loss 439.0950012207031\n",
      "Epoch 1357: New best model saved with Loss 437.42022705078125\n",
      "Epoch 1358: New best model saved with Loss 423.8564453125\n",
      "Epoch 1363: New best model saved with Loss 420.1683044433594\n",
      "Epoch 1364: New best model saved with Loss 415.1552429199219\n",
      "Epoch 1369: New best model saved with Loss 411.85498046875\n",
      "Epoch 1370: New best model saved with Loss 407.0162048339844\n",
      "Epoch 1375: New best model saved with Loss 406.988525390625\n",
      "Epoch 1376: New best model saved with Loss 401.6990051269531\n",
      "Epoch 1377: New best model saved with Loss 398.72808837890625\n",
      "Epoch 1378: New best model saved with Loss 398.5234069824219\n",
      "Epoch 1383: New best model saved with Loss 395.971435546875\n",
      "Epoch 1384: New best model saved with Loss 392.93951416015625\n",
      "Epoch 1385: New best model saved with Loss 390.543212890625\n",
      "Epoch 1386: New best model saved with Loss 389.0448913574219\n",
      "Epoch 1387: New best model saved with Loss 388.3343200683594\n",
      "Epoch 1388: New best model saved with Loss 388.1559753417969\n",
      "Epoch 1389: New best model saved with Loss 388.0624084472656\n",
      "Epoch 1390: New best model saved with Loss 387.8032531738281\n",
      "Epoch 1391: New best model saved with Loss 387.2063903808594\n",
      "Epoch 1392: New best model saved with Loss 386.2657470703125\n",
      "Epoch 1393: New best model saved with Loss 385.0293273925781\n",
      "Epoch 1394: New best model saved with Loss 383.59576416015625\n",
      "Epoch 1395: New best model saved with Loss 382.10931396484375\n",
      "Epoch 1396: New best model saved with Loss 380.65509033203125\n",
      "Epoch 1397: New best model saved with Loss 379.2744140625\n",
      "Epoch 1398: New best model saved with Loss 378.01458740234375\n",
      "Epoch 1399: New best model saved with Loss 376.83819580078125\n",
      "Epoch 1400: New best model saved with Loss 375.74609375\n",
      "Epoch 1401: New best model saved with Loss 374.7256774902344\n",
      "Epoch 1402: New best model saved with Loss 373.7533264160156\n",
      "Epoch 1403: New best model saved with Loss 372.80230712890625\n",
      "Epoch 1404: New best model saved with Loss 371.8764343261719\n",
      "Epoch 1405: New best model saved with Loss 371.0019836425781\n",
      "Epoch 1406: New best model saved with Loss 370.1138916015625\n",
      "Epoch 1407: New best model saved with Loss 369.25787353515625\n",
      "Epoch 1408: New best model saved with Loss 368.4327392578125\n",
      "Epoch 1409: New best model saved with Loss 367.6485595703125\n",
      "Epoch 1410: New best model saved with Loss 366.92022705078125\n",
      "Epoch 1411: New best model saved with Loss 366.2981262207031\n",
      "Epoch 1412: New best model saved with Loss 365.8642578125\n",
      "Epoch 1413: New best model saved with Loss 365.7682800292969\n",
      "Epoch 1476: New best model saved with Loss 361.30548095703125\n",
      "Epoch 1481: New best model saved with Loss 353.6197509765625\n",
      "Epoch 1486: New best model saved with Loss 347.637451171875\n",
      "Epoch 1491: New best model saved with Loss 347.4725341796875\n",
      "Epoch 1492: New best model saved with Loss 340.1560974121094\n",
      "Epoch 1497: New best model saved with Loss 339.95257568359375\n",
      "Epoch 1498: New best model saved with Loss 335.0124816894531\n",
      "Epoch 1499: New best model saved with Loss 332.5472106933594\n",
      "Epoch 1504: New best model saved with Loss 331.1725158691406\n",
      "Epoch 1505: New best model saved with Loss 328.1862487792969\n",
      "Epoch 1506: New best model saved with Loss 326.13641357421875\n",
      "Epoch 1507: New best model saved with Loss 325.5492248535156\n",
      "Epoch 1511: New best model saved with Loss 324.9867858886719\n",
      "Epoch 1512: New best model saved with Loss 323.50543212890625\n",
      "Epoch 1513: New best model saved with Loss 321.77880859375\n",
      "Epoch 1514: New best model saved with Loss 320.17413330078125\n",
      "Epoch 1515: New best model saved with Loss 318.80328369140625\n",
      "Epoch 1516: New best model saved with Loss 317.91717529296875\n",
      "Epoch 1517: New best model saved with Loss 317.28289794921875\n",
      "Epoch 1518: New best model saved with Loss 316.9420166015625\n",
      "Epoch 1519: New best model saved with Loss 316.5509033203125\n",
      "Epoch 1520: New best model saved with Loss 316.176025390625\n",
      "Epoch 1521: New best model saved with Loss 315.64422607421875\n",
      "Epoch 1522: New best model saved with Loss 314.9989318847656\n",
      "Epoch 1523: New best model saved with Loss 314.22515869140625\n",
      "Epoch 1524: New best model saved with Loss 313.37774658203125\n",
      "Epoch 1525: New best model saved with Loss 312.5044860839844\n",
      "Epoch 1526: New best model saved with Loss 311.5830383300781\n",
      "Epoch 1527: New best model saved with Loss 310.7184143066406\n",
      "Epoch 1528: New best model saved with Loss 309.851806640625\n",
      "Epoch 1529: New best model saved with Loss 309.0625305175781\n",
      "Epoch 1530: New best model saved with Loss 308.29376220703125\n",
      "Epoch 1531: New best model saved with Loss 307.57940673828125\n",
      "Epoch 1532: New best model saved with Loss 306.91021728515625\n",
      "Epoch 1533: New best model saved with Loss 306.2739562988281\n",
      "Epoch 1534: New best model saved with Loss 305.6962585449219\n",
      "Epoch 1535: New best model saved with Loss 305.1932678222656\n",
      "Epoch 1536: New best model saved with Loss 304.7820129394531\n",
      "Epoch 1537: New best model saved with Loss 304.5197448730469\n",
      "Epoch 1538: New best model saved with Loss 304.4953308105469\n",
      "Epoch 1602: New best model saved with Loss 302.4532165527344\n",
      "Epoch 1603: New best model saved with Loss 293.9234619140625\n",
      "Epoch 1609: New best model saved with Loss 291.4464416503906\n",
      "Epoch 1610: New best model saved with Loss 285.9493408203125\n",
      "Epoch 1618: New best model saved with Loss 283.1768798828125\n",
      "Epoch 1619: New best model saved with Loss 279.33544921875\n",
      "Epoch 1620: New best model saved with Loss 277.369873046875\n",
      "Epoch 1685: New best model saved with Loss 266.29425048828125\n",
      "Epoch 1695: New best model saved with Loss 265.9984130859375\n",
      "Epoch 1696: New best model saved with Loss 257.7041931152344\n",
      "Epoch 1697: New best model saved with Loss 255.3848419189453\n",
      "Epoch 1747: New best model saved with Loss 248.604736328125\n",
      "Epoch 1788: New best model saved with Loss 237.08084106445312\n",
      "Epoch 1812: New best model saved with Loss 231.51275634765625\n",
      "Epoch 1857: New best model saved with Loss 229.06729125976562\n",
      "Epoch 1858: New best model saved with Loss 222.83108520507812\n",
      "Epoch 1859: New best model saved with Loss 220.42234802246094\n",
      "Epoch 1928: New best model saved with Loss 219.67575073242188\n",
      "Epoch 1929: New best model saved with Loss 214.3228759765625\n",
      "Epoch 1930: New best model saved with Loss 212.20472717285156\n",
      "Epoch 1987: New best model saved with Loss 210.3610076904297\n",
      "Epoch 1988: New best model saved with Loss 206.55145263671875\n",
      "Epoch 2044: New best model saved with Loss 206.3253631591797\n",
      "Epoch 2054: New best model saved with Loss 198.07180786132812\n",
      "Epoch 2112: New best model saved with Loss 195.987548828125\n",
      "Epoch 2124: New best model saved with Loss 189.5940399169922\n",
      "Epoch 2125: New best model saved with Loss 186.9377899169922\n",
      "Epoch 2182: New best model saved with Loss 183.81060791015625\n",
      "Epoch 2183: New best model saved with Loss 180.4008331298828\n",
      "Epoch 2245: New best model saved with Loss 176.05809020996094\n",
      "Epoch 2246: New best model saved with Loss 175.95181274414062\n",
      "Epoch 2295: New best model saved with Loss 172.01544189453125\n",
      "Epoch 2405: New best model saved with Loss 164.26248168945312\n",
      "Epoch 2454: New best model saved with Loss 162.88600158691406\n",
      "Epoch 2491: New best model saved with Loss 158.91012573242188\n",
      "Epoch 2492: New best model saved with Loss 156.15582275390625\n",
      "Epoch 2493: New best model saved with Loss 154.63211059570312\n",
      "Epoch 2494: New best model saved with Loss 153.88592529296875\n",
      "Epoch 2495: New best model saved with Loss 153.5083465576172\n",
      "Epoch 2648: New best model saved with Loss 149.1127166748047\n",
      "Epoch 2649: New best model saved with Loss 147.53489685058594\n",
      "Epoch 2741: New best model saved with Loss 144.75497436523438\n",
      "Epoch 2742: New best model saved with Loss 141.82286071777344\n",
      "Epoch 2743: New best model saved with Loss 140.5764617919922\n",
      "Epoch 2878: New best model saved with Loss 138.94903564453125\n",
      "Epoch 2879: New best model saved with Loss 136.66937255859375\n",
      "Epoch 2931: New best model saved with Loss 136.236083984375\n",
      "Epoch 2984: New best model saved with Loss 134.8711395263672\n",
      "Epoch 2985: New best model saved with Loss 133.6607666015625\n",
      "Epoch 2986: New best model saved with Loss 133.07449340820312\n",
      "Epoch 2987: New best model saved with Loss 132.93038940429688\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10000,) and (3022,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 286\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    285\u001b[0m     ax\u001b[38;5;241m=\u001b[39mfig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m1\u001b[39m,n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_ylabel(namelist[n])\n\u001b[1;32m    288\u001b[0m ax\u001b[38;5;241m=\u001b[39mfig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m6\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/matplotlib/pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/matplotlib/axes/_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10000,) and (3022,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## warmup 阶段用大的学习率先快速下降loss\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import paddle.optimizer\n",
    "import logging\n",
    "import paddle.fft\n",
    "\n",
    "\n",
    "# 设置模型中的可调参数\n",
    "\n",
    "layerlist=[2,64,64,64,64,64,64,64,4]\n",
    "w=1 # boundary condition 的权重\n",
    "modes=12 # FFT特征保留多少分量\n",
    "# note='PINN_withhardBC'\n",
    "note='tPINN_resNet_FFT_ELU'\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "layerlist_str = \", \".join(map(str, layerlist))\n",
    "foldername = f\"{note}_w={w}_layers=[{layerlist_str}]\"\n",
    "\n",
    "folderpath=os.path.join(output_dir,foldername)\n",
    "os.makedirs(folderpath, exist_ok=True) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "paralist=[int(file[11:-9]) for file in os.listdir(os.path.join(output_dir,foldername)) if file.startswith('best_model_')]\n",
    "flag=False\n",
    "next=0\n",
    "if len(paralist)>0:\n",
    "    next=max(paralist)+1\n",
    "    flag=True\n",
    "\n",
    "saveparameterfile='best_model_'+str(next)+'.pdparams'\n",
    "if flag:\n",
    "    parameterfile='best_model_'+str(next-1)+'.pdparams'\n",
    "else:\n",
    "    parameterfile='best_model_0.pdparams'\n",
    "lossfigname=\"loss_curve\"+str(next)+'.png' \n",
    "\n",
    "log_file_path = os.path.join(output_dir, foldername, 'train'+str(next)+'.log')\n",
    "print(log_file_path)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,       # 日志文件名\n",
    "    filemode='a',               # 追加模式 (append mode)\n",
    "    level=logging.INFO,         # 设置日志级别（INFO、DEBUG等）\n",
    "    format='%(asctime)s %(levelname)s: %(message)s'  # 日志格式\n",
    ")\n",
    "\n",
    "class ELU(nn.Layer):\n",
    "    def forward(self, x):\n",
    "        mask = 0.5 * (paddle.sign(x) + 1)\n",
    "        # 限制 exp(x) 的输入范围，避免梯度爆炸\n",
    "        exp_x = paddle.exp(paddle.clip(x, max=10)) - 1\n",
    "        return mask * x + (1 - mask) * exp_x\n",
    "class ResNetBlock(nn.Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "        self.fc2 = nn.Linear(out_features, out_features)\n",
    "        self.activation = ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        out += identity  # Residual connection\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "# 定义神经网络结构，用于逼近解uvpt\n",
    "class tPINN(nn.Layer):\n",
    "    def __init__(self, layerlist,modes):\n",
    "        super(tPINN, self).__init__()\n",
    "        self.input_layer = nn.Linear(layerlist[0]+modes*2, layerlist[1])\n",
    "        self.res_blocks = nn.LayerList([\n",
    "            ResNetBlock(layerlist[i], layerlist[i+1]) for i in range(1,len(layerlist) - 2)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(layerlist[-2], layerlist[-1])\n",
    "        \n",
    "    def forward(self, x, y,modes):\n",
    "        a=1\n",
    "        # 计算 x 和 y 坐标的 FFT 特征\n",
    "        x_fft = paddle.real(paddle.fft.fft(x,modes))\n",
    "        y_fft = paddle.real(paddle.fft.fft(y,modes))\n",
    "        \n",
    "        # 将 x, y 和它们的 FFT 特征堆叠\n",
    "        xy_fft = paddle.concat([x, y, x_fft, y_fft], axis=1)\n",
    "        \n",
    "        # 将堆叠的结果作为输入\n",
    "        out = self.input_layer(xy_fft)\n",
    "        for res_block in self.res_blocks:\n",
    "            out = res_block(out)\n",
    "        result = self.output_layer(out)\n",
    "\n",
    "        x = xy_fft[:, 0]\n",
    "        y = xy_fft[:, 1]\n",
    "        u = result[:, 0]\n",
    "        v = result[:, 1]\n",
    "        p = result[:, 2]\n",
    "        T = result[:, 3]\n",
    "        \n",
    "        u = x * (1 - x) * y * (1 - y) * u\n",
    "        v = x * (1 - x) * y * (1 - y) * v\n",
    "        T = 1 - x + paddle.sin(paddle.to_tensor(np.pi) * x) * T\n",
    "        uvpt = paddle.concat([u.unsqueeze(1), v.unsqueeze(1), p.unsqueeze(1), T.unsqueeze(1)], axis=1)\n",
    "        return uvpt\n",
    "\n",
    "        \n",
    "def generate_data(num_points):\n",
    "    x_data = np.linspace(0, 1, num_points)\n",
    "    y_data = np.linspace(0, 1, num_points)\n",
    "    # xy_data_boundary=np.linspace(0, 1, num_boundary_points)\n",
    "\n",
    "\n",
    "    X, Y = np.meshgrid(x_data, y_data)\n",
    "\n",
    "    x_train=paddle.to_tensor(X.flatten()[:, None], dtype='float32')\n",
    "    y_train=paddle.to_tensor(Y.flatten()[:, None], dtype='float32')\n",
    "\n",
    "    return x_train,y_train\n",
    "def conv_derivative(u, axis=0, dx=1e-5, verbose=False):\n",
    "    \"\"\" 使用有限差分进行计算\"\"\"\n",
    "    if len(u.shape) != 2:\n",
    "        raise ValueError(\"输入张量 u 必须是二维的\")\n",
    "    result=0*u\n",
    "    if axis==0:\n",
    "        result[:,1:-1]=(u[:,2:]-u[:,:-2])/(2*dx)\n",
    "        result[:,0]=(4*u[:,1]-3*u[:,0]-u[:,2])/(2*dx)\n",
    "        result[:,-1]=(3*u[:,-1]-4*u[:,-2]+u[:,-3])/(2*dx)\n",
    "    else:\n",
    "        result[1:-1,:]=(u[2:,:]-u[:-2,:])/(2*dx)\n",
    "        result[0,:]=(4*u[1,:]-3*u[0,:]-u[2,:])/(2*dx)\n",
    "        result[-1,:]=(3*u[-1,:]-4*u[-2,:]+u[-3,:])/(2*dx)\n",
    "    return result\n",
    "\n",
    "\n",
    "def conv_derivative_2(u, axis=0, dx=1e-5, verbose=False):\n",
    "    \"\"\" 使用有限差分进行计算\"\"\"\n",
    "    if len(u.shape) != 2:\n",
    "        raise ValueError(\"输入张量 u 必须是二维的\")\n",
    "    result=0*u\n",
    "    if axis==0:\n",
    "        result[:,1:-1]=(u[:,2:]-2*u[:,1:-1]+u[:,:-2])/(dx*dx)\n",
    "        result[:,0]=(2*u[:,0]-5*u[:,1]+4*u[:,2]-u[:,3])/(dx*dx)\n",
    "        result[:,-1]=(2*u[:,-1]-5*u[:,-2]+4*u[:,-3]-u[:,-4])/(dx*dx)\n",
    "    else:\n",
    "        result[1:-1,:]=(u[2:,:]-2*u[1:-1,:]+u[:-2,:])/(dx*dx)\n",
    "        result[0,:]=(2*u[0,:]-5*u[1,:]+4*u[2,:]-u[3,:])/(dx*dx)\n",
    "        result[-1,:]=(2*u[-1,:]-5*u[-2,:]+4*u[-3,:]-u[-4,:])/(dx*dx)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 定义微分函数，用于计算 PDE 中的导数\n",
    "def compute_pde_loss(x,y):\n",
    "    Pr=0.7\n",
    "    Ra=1000\n",
    "    N=num_points+1\n",
    "    dx=1/(N)\n",
    "\n",
    "    uvpt = pinn_model(x, y,modes)\n",
    "    u, v, p, T = uvpt[:, 0:1], uvpt[:, 1:2], uvpt[:, 2:3], uvpt[:, 3:4]\n",
    "    u=paddle.reshape(u,[N,N])\n",
    "    v=paddle.reshape(v,[N,N])\n",
    "    p=paddle.reshape(p,[N,N])\n",
    "    T=paddle.reshape(T,[N,N])\n",
    "    \n",
    "    u_x = conv_derivative(u, axis=0,dx=dx)\n",
    "    u_y = conv_derivative(u, axis=1,dx=dx)\n",
    "    v_x = conv_derivative(v, axis=0,dx=dx)\n",
    "    v_y = conv_derivative(v, axis=1,dx=dx)\n",
    "    T_x = conv_derivative(T, axis=0,dx=dx)\n",
    "    T_y = conv_derivative(T, axis=1,dx=dx)\n",
    "    p_x = conv_derivative(p, axis=0,dx=dx)\n",
    "    p_y = conv_derivative(p, axis=1,dx=dx)\n",
    "    \n",
    "    u_xx = conv_derivative_2(u, axis=0,dx=dx)\n",
    "    u_yy = conv_derivative_2(u, axis=1,dx=dx)\n",
    "    v_xx = conv_derivative_2(v, axis=0,dx=dx)\n",
    "    v_yy = conv_derivative_2(v, axis=1,dx=dx)\n",
    "    T_xx = conv_derivative_2(T, axis=0,dx=dx)\n",
    "    T_yy = conv_derivative_2(T, axis=1,dx=dx)\n",
    "    # 方程\n",
    "    continuity_residual = (u_x + v_y)[1:-1,1:-1].flatten()\n",
    "    momentum_u_residual = (u * u_x + v * u_y - Pr * (u_xx + u_yy) + p_x)[1:-1,1:-1].flatten()\n",
    "    momentum_v_residual = (u * v_x + v * v_y - Pr * (v_xx + v_yy) + p_y -Pr * Ra * T)[1:-1,1:-1].flatten()\n",
    "    energy_residual = (u * T_x + v * T_y- (T_xx + T_yy))[1:-1,1:-1].flatten()\n",
    "\n",
    "\n",
    "    left=((p_x-Pr*u_xx)[:,0])**2\n",
    "    right=((p_x-Pr*u_xx)[:,-1])**2\n",
    "    PrRaT=Pr * Ra * T\n",
    "    top=(T_y[-1,:])**2+((p_y-Pr*v_yy-PrRaT)[-1,:])**2\n",
    "    bottom=(T_y[0,:])**2+((p_y-Pr*v_yy-PrRaT)[0,:])**2\n",
    "\n",
    "    boundary_residual=paddle.mean(left)+paddle.mean(right)+paddle.mean(top)+paddle.mean(bottom)\n",
    "    return continuity_residual, momentum_u_residual, momentum_v_residual, energy_residual,boundary_residual\n",
    "\n",
    "\n",
    "\n",
    "pinn_model = tPINN(layerlist=layerlist,modes=modes)  # 使用新的网络结构\n",
    "if os.path.exists(os.path.join(output_dir,foldername,parameterfile)):\n",
    "    pinn_model.set_state_dict(paddle.load(os.path.join(output_dir, foldername,parameterfile)))\n",
    "    pinn_model.train()\n",
    "\n",
    "num_points=200\n",
    "num_boundary_points=4000\n",
    "x_train,y_train= generate_data(num_points+1)\n",
    "logging.info(f\"{note}\")\n",
    "logging.info(f\"num of point {num_points}, layer={layerlist_str},weight of boundary={w}\")\n",
    "\n",
    "# scheduler=paddle.optimizer.lr.CosineAnnealingWarmRestarts(learning_rate=0.005,T_0=50)\n",
    "# clip=paddle.nn.ClipGradByGlobalNorm(clip_norm=2.0)\n",
    "optimizer = paddle.optimizer.Adam(parameters=pinn_model.parameters(), learning_rate=0.01)\n",
    "patience=0\n",
    "epochs = 10000\n",
    "losses=[]\n",
    "best_loss = float('inf')\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.clear_grad()\n",
    "\n",
    "    # 计算PDE损失\n",
    "    continuity_residual, momentum_u_residual, momentum_v_residual, energy_residual, boundary_residual = compute_pde_loss(x_train, y_train)\n",
    "    loss = (\n",
    "            paddle.mean(continuity_residual**2) +\n",
    "            paddle.mean(momentum_u_residual**2) +\n",
    "            paddle.mean(momentum_v_residual**2) +\n",
    "            paddle.mean(energy_residual**2) +\n",
    "            boundary_residual * w\n",
    "        )\n",
    "    # 反向传播和优化\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append([\n",
    "    paddle.mean(continuity_residual ** 2).numpy(),\n",
    "    paddle.mean(momentum_u_residual ** 2).numpy(),\n",
    "    paddle.mean(momentum_v_residual ** 2).numpy(),\n",
    "    paddle.mean(energy_residual ** 2).numpy(),\n",
    "    (boundary_residual * w).numpy()])\n",
    "\n",
    "    \n",
    "    if loss.numpy() < best_loss:\n",
    "        best_loss = loss.numpy()\n",
    "        patience=0\n",
    "        paddle.save(pinn_model.state_dict(), os.path.join(output_dir,foldername, saveparameterfile))\n",
    "        print(f\"Epoch {epoch}: New best model saved with Loss {best_loss}\")\n",
    "        logging.info(f\"Epoch {epoch}: New best model saved with Loss {best_loss}\")\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch}: loss {loss.numpy():.6f}, continuity {losses[-1][0]:.6f}, \"\n",
    "            f\"momentum_u {losses[-1][1]:.6f}, momentum_v {losses[-1][2]:.6f}, \"\n",
    "            f\"energy {losses[-1][3]:.6f}, boundary {losses[-1][4]:.6f}\")\n",
    "\n",
    "    else:\n",
    "        patience+=1\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch}: loss {loss.numpy():.6f}, continuity {losses[-1][0]:.6f}, \"\n",
    "            f\"momentum_u {losses[-1][1]:.6f}, momentum_v {losses[-1][2]:.6f}, \"\n",
    "            f\"energy {losses[-1][3]:.6f}, boundary {losses[-1][4]:.6f}\")\n",
    "    if patience>5000 and loss.numpy()<best_loss*1.1:\n",
    "        current_lr = optimizer.get_lr()\n",
    "        optimizer.set_lr(max(current_lr*0.5,1e-6))\n",
    "        print(f\"Learning rate reduced  from {current_lr:.6f} to {current_lr*0.5:.6f}\")\n",
    "        logging.info(f\"Learning rate reduced  from {current_lr:.6f} to {current_lr*0.5:.6f}\")\n",
    "        patience=0\n",
    "    if loss.numpy() >10000*best_loss:\n",
    "        break\n",
    "\n",
    "namelist=['continuity','momentum_u','momentum_v','energy','boundary']\n",
    "fig=plt.figure(figsize=(10,20))\n",
    "for n in range(5):\n",
    "    ax=fig.add_subplot(6,1,n+1)\n",
    "    plt.plot(range(epochs), np.log10(np.array(losses)[:,n]))\n",
    "    ax.set_ylabel(namelist[n])\n",
    "ax=fig.add_subplot(6,1,6)\n",
    "plt.plot(range(epochs), np.log10(np.sum(np.array(losses),axis=1)), label='total')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "loss_curve_path = os.path.join(output_dir,foldername, lossfigname)\n",
    "plt.savefig(loss_curve_path)\n",
    "plt.close()\n",
    "logging.info(f\"Loss 曲线已保存到 {loss_curve_path}\")\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d328bcf-dedc-4b02-901b-88f48e028ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/tPINN_resNet_FFT_ELU_w=1_layers=[2, 64, 64, 64, 64, 64, 64, 64, 4]/train2.log\n",
      "Epoch 0: New best model saved with Loss 0.10313453525304794\n",
      "Epoch 80: New best model saved with Loss 0.10312477499246597\n",
      "Epoch 96: New best model saved with Loss 0.10309676080942154\n",
      "Epoch 112: New best model saved with Loss 0.10294969379901886\n",
      "Epoch 136: New best model saved with Loss 0.10294699668884277\n",
      "Epoch 169: New best model saved with Loss 0.10287412256002426\n",
      "Learning rate reduced  from 0.000010 to 0.000005\n",
      "Epoch 5201: New best model saved with Loss 0.10273648798465729\n",
      "Epoch 5219: New best model saved with Loss 0.10268910974264145\n",
      "Epoch 5271: New best model saved with Loss 0.10267330706119537\n",
      "Epoch 5280: New best model saved with Loss 0.10248980671167374\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import paddle.optimizer\n",
    "import logging\n",
    "import paddle.fft\n",
    "\n",
    "\n",
    "# 设置模型中的可调参数\n",
    "\n",
    "layerlist=[2,64,64,64,64,64,64,64,4]\n",
    "w=1 # boundary condition 的权重\n",
    "modes=12 # FFT特征保留多少分量\n",
    "# note='PINN_withhardBC'\n",
    "note='tPINN_resNet_FFT_ELU'\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "layerlist_str = \", \".join(map(str, layerlist))\n",
    "foldername = f\"{note}_w={w}_layers=[{layerlist_str}]\"\n",
    "\n",
    "folderpath=os.path.join(output_dir,foldername)\n",
    "os.makedirs(folderpath, exist_ok=True) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "paralist=[int(file[11:-9]) for file in os.listdir(os.path.join(output_dir,foldername)) if file.startswith('best_model_')]\n",
    "flag=False\n",
    "next=0\n",
    "if len(paralist)>0:\n",
    "    next=max(paralist)+1\n",
    "    flag=True\n",
    "\n",
    "saveparameterfile='best_model_'+str(next)+'.pdparams'\n",
    "if flag:\n",
    "    parameterfile='best_model_'+str(next-1)+'.pdparams'\n",
    "else:\n",
    "    parameterfile='best_model_0.pdparams'\n",
    "lossfigname=\"loss_curve\"+str(next)+'.png' \n",
    "\n",
    "log_file_path = os.path.join(output_dir, foldername, 'train'+str(next)+'.log')\n",
    "print(log_file_path)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,       # 日志文件名\n",
    "    filemode='a',               # 追加模式 (append mode)\n",
    "    level=logging.INFO,         # 设置日志级别（INFO、DEBUG等）\n",
    "    format='%(asctime)s %(levelname)s: %(message)s'  # 日志格式\n",
    ")\n",
    "\n",
    "class ELU(nn.Layer):\n",
    "    def forward(self, x):\n",
    "        mask = 0.5 * (paddle.sign(x) + 1)\n",
    "        # 限制 exp(x) 的输入范围，避免梯度爆炸\n",
    "        exp_x = paddle.exp(paddle.clip(x, max=10)) - 1\n",
    "        return mask * x + (1 - mask) * exp_x\n",
    "class ResNetBlock(nn.Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "        self.fc2 = nn.Linear(out_features, out_features)\n",
    "        self.activation = ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        out += identity  # Residual connection\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "# 定义神经网络结构，用于逼近解uvpt\n",
    "class tPINN(nn.Layer):\n",
    "    def __init__(self, layerlist,modes):\n",
    "        super(tPINN, self).__init__()\n",
    "        self.input_layer = nn.Linear(layerlist[0]+modes*2, layerlist[1])\n",
    "        self.res_blocks = nn.LayerList([\n",
    "            ResNetBlock(layerlist[i], layerlist[i+1]) for i in range(1,len(layerlist) - 2)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(layerlist[-2], layerlist[-1])\n",
    "        \n",
    "    def forward(self, x, y,modes):\n",
    "        a=1\n",
    "        # 计算 x 和 y 坐标的 FFT 特征\n",
    "        x_fft = paddle.real(paddle.fft.fft(x,modes))\n",
    "        y_fft = paddle.real(paddle.fft.fft(y,modes))\n",
    "        \n",
    "        # 将 x, y 和它们的 FFT 特征堆叠\n",
    "        xy_fft = paddle.concat([x, y, x_fft, y_fft], axis=1)\n",
    "        \n",
    "        # 将堆叠的结果作为输入\n",
    "        out = self.input_layer(xy_fft)\n",
    "        for res_block in self.res_blocks:\n",
    "            out = res_block(out)\n",
    "        result = self.output_layer(out)\n",
    "\n",
    "        x = xy_fft[:, 0]\n",
    "        y = xy_fft[:, 1]\n",
    "        u = result[:, 0]\n",
    "        v = result[:, 1]\n",
    "        p = result[:, 2]\n",
    "        T = result[:, 3]\n",
    "        \n",
    "        u = x * (1 - x) * y * (1 - y) * u\n",
    "        v = x * (1 - x) * y * (1 - y) * v\n",
    "        T = 1 - x + paddle.sin(paddle.to_tensor(np.pi) * x) * T\n",
    "        uvpt = paddle.concat([u.unsqueeze(1), v.unsqueeze(1), p.unsqueeze(1), T.unsqueeze(1)], axis=1)\n",
    "        return uvpt\n",
    "\n",
    "        \n",
    "def generate_data(num_points):\n",
    "    x_data = np.linspace(0, 1, num_points)\n",
    "    y_data = np.linspace(0, 1, num_points)\n",
    "    # xy_data_boundary=np.linspace(0, 1, num_boundary_points)\n",
    "\n",
    "\n",
    "    X, Y = np.meshgrid(x_data, y_data)\n",
    "\n",
    "    x_train=paddle.to_tensor(X.flatten()[:, None], dtype='float32')\n",
    "    y_train=paddle.to_tensor(Y.flatten()[:, None], dtype='float32')\n",
    "\n",
    "    return x_train,y_train\n",
    "def conv_derivative(u, axis=0, dx=1e-5, verbose=False):\n",
    "    \"\"\" 使用有限差分进行计算\"\"\"\n",
    "    if len(u.shape) != 2:\n",
    "        raise ValueError(\"输入张量 u 必须是二维的\")\n",
    "    result=0*u\n",
    "    if axis==0:\n",
    "        result[:,1:-1]=(u[:,2:]-u[:,:-2])/(2*dx)\n",
    "        result[:,0]=(4*u[:,1]-3*u[:,0]-u[:,2])/(2*dx)\n",
    "        result[:,-1]=(3*u[:,-1]-4*u[:,-2]+u[:,-3])/(2*dx)\n",
    "    else:\n",
    "        result[1:-1,:]=(u[2:,:]-u[:-2,:])/(2*dx)\n",
    "        result[0,:]=(4*u[1,:]-3*u[0,:]-u[2,:])/(2*dx)\n",
    "        result[-1,:]=(3*u[-1,:]-4*u[-2,:]+u[-3,:])/(2*dx)\n",
    "    return result\n",
    "\n",
    "\n",
    "def conv_derivative_2(u, axis=0, dx=1e-5, verbose=False):\n",
    "    \"\"\" 使用有限差分进行计算\"\"\"\n",
    "    if len(u.shape) != 2:\n",
    "        raise ValueError(\"输入张量 u 必须是二维的\")\n",
    "    result=0*u\n",
    "    if axis==0:\n",
    "        result[:,1:-1]=(u[:,2:]-2*u[:,1:-1]+u[:,:-2])/(dx*dx)\n",
    "        result[:,0]=(2*u[:,0]-5*u[:,1]+4*u[:,2]-u[:,3])/(dx*dx)\n",
    "        result[:,-1]=(2*u[:,-1]-5*u[:,-2]+4*u[:,-3]-u[:,-4])/(dx*dx)\n",
    "    else:\n",
    "        result[1:-1,:]=(u[2:,:]-2*u[1:-1,:]+u[:-2,:])/(dx*dx)\n",
    "        result[0,:]=(2*u[0,:]-5*u[1,:]+4*u[2,:]-u[3,:])/(dx*dx)\n",
    "        result[-1,:]=(2*u[-1,:]-5*u[-2,:]+4*u[-3,:]-u[-4,:])/(dx*dx)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 定义微分函数，用于计算 PDE 中的导数\n",
    "def compute_pde_loss(x,y):\n",
    "    Pr=0.7\n",
    "    Ra=1000\n",
    "    N=num_points+1\n",
    "    dx=1/(N)\n",
    "\n",
    "    uvpt = pinn_model(x, y,modes)\n",
    "    u, v, p, T = uvpt[:, 0:1], uvpt[:, 1:2], uvpt[:, 2:3], uvpt[:, 3:4]\n",
    "    u=paddle.reshape(u,[N,N])\n",
    "    v=paddle.reshape(v,[N,N])\n",
    "    p=paddle.reshape(p,[N,N])\n",
    "    T=paddle.reshape(T,[N,N])\n",
    "    \n",
    "    u_x = conv_derivative(u, axis=0,dx=dx)\n",
    "    u_y = conv_derivative(u, axis=1,dx=dx)\n",
    "    v_x = conv_derivative(v, axis=0,dx=dx)\n",
    "    v_y = conv_derivative(v, axis=1,dx=dx)\n",
    "    T_x = conv_derivative(T, axis=0,dx=dx)\n",
    "    T_y = conv_derivative(T, axis=1,dx=dx)\n",
    "    p_x = conv_derivative(p, axis=0,dx=dx)\n",
    "    p_y = conv_derivative(p, axis=1,dx=dx)\n",
    "    \n",
    "    u_xx = conv_derivative_2(u, axis=0,dx=dx)\n",
    "    u_yy = conv_derivative_2(u, axis=1,dx=dx)\n",
    "    v_xx = conv_derivative_2(v, axis=0,dx=dx)\n",
    "    v_yy = conv_derivative_2(v, axis=1,dx=dx)\n",
    "    T_xx = conv_derivative_2(T, axis=0,dx=dx)\n",
    "    T_yy = conv_derivative_2(T, axis=1,dx=dx)\n",
    "    # 方程\n",
    "    continuity_residual = (u_x + v_y)[1:-1,1:-1].flatten()\n",
    "    momentum_u_residual = (u * u_x + v * u_y - Pr * (u_xx + u_yy) + p_x)[1:-1,1:-1].flatten()\n",
    "    momentum_v_residual = (u * v_x + v * v_y - Pr * (v_xx + v_yy) + p_y -Pr * Ra * T)[1:-1,1:-1].flatten()\n",
    "    energy_residual = (u * T_x + v * T_y- (T_xx + T_yy))[1:-1,1:-1].flatten()\n",
    "\n",
    "\n",
    "    left=((p_x-Pr*u_xx)[:,0])**2\n",
    "    right=((p_x-Pr*u_xx)[:,-1])**2\n",
    "    PrRaT=Pr * Ra * T\n",
    "    top=(T_y[-1,:])**2+((p_y-Pr*v_yy-PrRaT)[-1,:])**2\n",
    "    bottom=(T_y[0,:])**2+((p_y-Pr*v_yy-PrRaT)[0,:])**2\n",
    "\n",
    "    boundary_residual=paddle.mean(left)+paddle.mean(right)+paddle.mean(top)+paddle.mean(bottom)\n",
    "    return continuity_residual, momentum_u_residual, momentum_v_residual, energy_residual,boundary_residual\n",
    "\n",
    "\n",
    "\n",
    "pinn_model = tPINN(layerlist=layerlist,modes=modes)  # 使用新的网络结构\n",
    "if os.path.exists(os.path.join(output_dir,foldername,parameterfile)):\n",
    "    pinn_model.set_state_dict(paddle.load(os.path.join(output_dir, foldername,parameterfile)))\n",
    "    pinn_model.train()\n",
    "\n",
    "num_points=200\n",
    "num_boundary_points=4000\n",
    "x_train,y_train= generate_data(num_points+1)\n",
    "logging.info(f\"{note}\")\n",
    "logging.info(f\"num of point {num_points}, layer={layerlist_str},weight of boundary={w}\")\n",
    "\n",
    "# scheduler=paddle.optimizer.lr.CosineAnnealingWarmRestarts(learning_rate=0.005,T_0=50)\n",
    "# clip=paddle.nn.ClipGradByGlobalNorm(clip_norm=2.0)\n",
    "optimizer = paddle.optimizer.Adam(parameters=pinn_model.parameters(), learning_rate=1e-5)\n",
    "patience=0\n",
    "epochs = 1000000\n",
    "losses=[]\n",
    "best_loss = float('inf')\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.clear_grad()\n",
    "\n",
    "    # 计算PDE损失\n",
    "    continuity_residual, momentum_u_residual, momentum_v_residual, energy_residual, boundary_residual = compute_pde_loss(x_train, y_train)\n",
    "    loss = (\n",
    "            paddle.mean(continuity_residual**2) +\n",
    "            paddle.mean(momentum_u_residual**2) +\n",
    "            paddle.mean(momentum_v_residual**2) +\n",
    "            paddle.mean(energy_residual**2) +\n",
    "            boundary_residual * w\n",
    "        )\n",
    "    # 反向传播和优化\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append([\n",
    "    paddle.mean(continuity_residual ** 2).numpy(),\n",
    "    paddle.mean(momentum_u_residual ** 2).numpy(),\n",
    "    paddle.mean(momentum_v_residual ** 2).numpy(),\n",
    "    paddle.mean(energy_residual ** 2).numpy(),\n",
    "    (boundary_residual * w).numpy()])\n",
    "\n",
    "    \n",
    "    if loss.numpy() < best_loss:\n",
    "        best_loss = loss.numpy()\n",
    "        patience=0\n",
    "        paddle.save(pinn_model.state_dict(), os.path.join(output_dir,foldername, saveparameterfile))\n",
    "        print(f\"Epoch {epoch}: New best model saved with Loss {best_loss}\")\n",
    "        logging.info(f\"Epoch {epoch}: New best model saved with Loss {best_loss}\")\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch}: loss {loss.numpy():.6f}, continuity {losses[-1][0]:.6f}, \"\n",
    "            f\"momentum_u {losses[-1][1]:.6f}, momentum_v {losses[-1][2]:.6f}, \"\n",
    "            f\"energy {losses[-1][3]:.6f}, boundary {losses[-1][4]:.6f}\")\n",
    "\n",
    "    else:\n",
    "        patience+=1\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch}: loss {loss.numpy():.6f}, continuity {losses[-1][0]:.6f}, \"\n",
    "            f\"momentum_u {losses[-1][1]:.6f}, momentum_v {losses[-1][2]:.6f}, \"\n",
    "            f\"energy {losses[-1][3]:.6f}, boundary {losses[-1][4]:.6f}\")\n",
    "    if patience>5000 and loss.numpy()<best_loss*1.1:\n",
    "        current_lr = optimizer.get_lr()\n",
    "        optimizer.set_lr(max(current_lr*0.5,1e-6))\n",
    "        print(f\"Learning rate reduced  from {current_lr:.6f} to {max(current_lr*0.5,1e-6):.6f}\")\n",
    "        logging.info(f\"Learning rate reduced  from {current_lr:.6f} to {max(current_lr*0.5,1e-6):.6f}\")\n",
    "        patience=0\n",
    "\n",
    "\n",
    "namelist=['continuity','momentum_u','momentum_v','energy','boundary']\n",
    "fig=plt.figure(figsize=(10,20))\n",
    "for n in range(5):\n",
    "    ax=fig.add_subplot(6,1,n+1)\n",
    "    plt.plot(range(epochs), np.log10(np.array(losses)[:,n]))\n",
    "    ax.set_ylabel(namelist[n])\n",
    "ax=fig.add_subplot(6,1,6)\n",
    "plt.plot(range(epochs), np.log10(np.sum(np.array(losses),axis=1)), label='total')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "loss_curve_path = os.path.join(output_dir,foldername, lossfigname)\n",
    "plt.savefig(loss_curve_path)\n",
    "plt.close()\n",
    "logging.info(f\"Loss 曲线已保存到 {loss_curve_path}\")\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30f7c7-cf0b-4943-8cbf-bf4f580c6fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
